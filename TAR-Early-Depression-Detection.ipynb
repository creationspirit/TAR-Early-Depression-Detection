{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from itertools import combinations, chain\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from lxml import etree\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag, FreqDist\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFECV, SelectFromModel\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n",
    "\n",
    "from scipy.sparse import vstack, hstack, coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from empath import Empath\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle up all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\\nX_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\\nX_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\\nX_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\\nX_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\\n\\nX_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\\nX_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\\nX_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\\nX_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\\nX_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\\n\\nX_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\\nX_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\\n\\nX_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\\nX_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\n",
    "X_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\n",
    "X_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\n",
    "X_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\n",
    "X_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\n",
    "\n",
    "X_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\n",
    "X_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\n",
    "X_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\n",
    "X_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\n",
    "X_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\n",
    "\n",
    "X_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\n",
    "X_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\n",
    "\n",
    "X_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\n",
    "X_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Corpus preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_diff = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'he', 'him', 'his', 'himself', \n",
    "           'she', 'her', 'hers', 'herself', 'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', \n",
    "           'themselves', 'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "           'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', 'totally', 'whole',\n",
    "           'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.stopwords.difference_update(sw_diff)\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X, method='lem'):\n",
    "        return [\n",
    "            list(self.tokenize(doc, method)) for doc in X\n",
    "        ]   \n",
    "\n",
    "    def tokenize(self, document, method='lem'):\n",
    "        if(method == 'lem'):\n",
    "            # Break the document into sentences\n",
    "            for sent in sent_tokenize(document):\n",
    "                # Break the sentence into part of speech tagged tokens\n",
    "                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                    # Apply preprocessing to the token\n",
    "                    token = self.process_token(token)\n",
    "                    if not self.is_valid_token(token):\n",
    "                        continue\n",
    "                        \n",
    "                    # Lemmatize the token and yield\n",
    "                    lemma = self.lemmatize(token, tag)\n",
    "                    yield lemma\n",
    "                    \n",
    "        elif(method == 'stem'):\n",
    "            # Break the document into tokens\n",
    "            for token in wordpunct_tokenize(document):\n",
    "                # Apply preprocessing to the token\n",
    "                token = self.process_token(token)\n",
    "                if not self.is_valid_token(token):\n",
    "                    continue\n",
    "                \n",
    "                stem = self.stem(token)\n",
    "                yield stem\n",
    "        else:\n",
    "            raise ValueError('Unknown method type.')\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def stem(self, token):\n",
    "        return self.stemmer.stem(token)\n",
    "    \n",
    "    def process_token(self, token):\n",
    "        token = token.lower() if self.lower else token\n",
    "        token = token.strip() if self.strip else tcharoken\n",
    "        token = token.strip('_') if self.strip else token\n",
    "        token = token.strip('*') if self.strip else token\n",
    "        return token\n",
    "    \n",
    "    def is_valid_token(self, token):\n",
    "        # If stopword, token is invalid\n",
    "        if token in self.stopwords:\n",
    "            return False\n",
    "\n",
    "        # If punctuation, token is invalid\n",
    "        if all(char in self.punct for char in token):\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code loads data corpus from multiple files into lists X (texts) and y(labels) with one entry per user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entries(X, y, path_list, label_dict=None, default_label=0):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "    \n",
    "    IMAGE_STR = 'data:image'\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "        \n",
    "            user_text = ''\n",
    "            for post in root.findall('.//TITLE') + root.findall('.//TEXT'):\n",
    "                post = post.text.strip().strip()\n",
    "                if post != '':\n",
    "                    if IMAGE_STR in post:\n",
    "                        continue\n",
    "                    post = re.sub(r\"http\\S+\", \" \", post)\n",
    "                    post = re.sub(r\"\\d+\", \" \", post)\n",
    "                    post = re.sub(u\"\\xa0\", \" \", post)\n",
    "                    post = re.sub(u\"\\\\p{P}+\", \" \", post)\n",
    "                    user_text += ' ' + post.lower()\n",
    "            \n",
    "            X.append(user_text)\n",
    "            label = int(label_dict[user_id]) if label_dict else default_label\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility methods for extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sentence_length(sentences):\n",
    "    sum = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(' ', '')\n",
    "        sum += len(sentence)\n",
    "    return sum / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_and_subjectivity(sentences):\n",
    "    sum_sentiment = 0\n",
    "    sum_subjectivity = 0\n",
    "    if len(sentences) > 0:\n",
    "        for sentence in sentences:\n",
    "            tb = TextBlob(sentence)\n",
    "            sum_sentiment += tb.sentiment.polarity\n",
    "            sum_subjectivity += tb.sentiment.subjectivity\n",
    "        sum_sentiment = sum_sentiment / float(len(sentences))\n",
    "        sum_subjectivity = sum_subjectivity / float(len(sentences))\n",
    "        return (sum_sentiment, sum_subjectivity)\n",
    "    else:\n",
    "        return (0.0, 0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(X_times, X_sentence_lengths, X_post_cnt, X_sentiment,\n",
    "                  X_subjectivity, X_post_lengths, X_post_freq, path_list):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "        \n",
    "    IMAGE_STR = 'data:image'\n",
    "    datetime_pattern = '%Y-%m-%d %H:%M:%S'\n",
    "    date_end = None\n",
    "    date_start = None\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "            \n",
    "            sentences = []\n",
    "            post_lengths = []\n",
    "            post_cnt = 0\n",
    "            for post in root.findall('.//TEXT'):\n",
    "                post_cnt += 1\n",
    "                post = post.text.strip()\n",
    "                if post != '':\n",
    "                    sentences.extend(sent_tokenize(post))\n",
    "                    post_lengths.append(len(post))\n",
    "                else:\n",
    "                    post_lengths.append(0)\n",
    "            \n",
    "            avg_sentiment, avg_subjectivity = get_sentiment_and_subjectivity(sentences)\n",
    "            avg_sentence_length = get_avg_sentence_length(sentences)\n",
    "            avg_post_length = np.mean(post_lengths)\n",
    "            \n",
    "            sum_hours = 0\n",
    "            for date in root.findall('.//DATE'):\n",
    "                date = date.text.strip()\n",
    "                if date != '':                    \n",
    "                    if not date_end:\n",
    "                        date_end = datetime.strptime(date, datetime_pattern)\n",
    "                    date_start = datetime.strptime(date, datetime_pattern)\n",
    "                    \n",
    "                    m = re.match(r'\\d{4}-\\d{2}-\\d{2} (\\d{2}).*', date)\n",
    "                    hour = int(m.group(1))\n",
    "                    sum_hours += hour\n",
    "            \n",
    "            post_span_minutes = (date_end - date_start).total_seconds()/60\n",
    "            post_freq = post_span_minutes / post_cnt\n",
    "            \n",
    "            time = [0] * 8\n",
    "            avg_hour = sum_hours / post_cnt\n",
    "            index = int(avg_hour // 3)\n",
    "            time[index] = 1\n",
    "            \n",
    "            X_post_cnt.append([post_cnt])\n",
    "            X_sentence_lengths.append([avg_sentence_length])\n",
    "            X_times.append(time)\n",
    "            X_sentiment.append([avg_sentiment])\n",
    "            X_subjectivity.append([avg_subjectivity])\n",
    "            X_post_lengths.append([avg_post_length])\n",
    "            X_post_freq.append([post_freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "TRAIN_PATH = os.path.join(cwd, \"reddit-training-ready-to-share\")\n",
    "TEST_PATH = os.path.join(cwd, \"reddit-test-data-ready-to-share\")\n",
    "\n",
    "TRAIN_POSITIVE_PATH = os.path.join(TRAIN_PATH, \"positive_examples_anonymous\")\n",
    "TRAIN_NEGATIVE_PATH = os.path.join(TRAIN_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TEST_POSITIVE_PATH = os.path.join(TEST_PATH, \"positive_examples_anonymous\")\n",
    "TEST_NEGATIVE_PATH = os.path.join(TEST_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TRAIN_LABELS_PATH = os.path.join(cwd, 'risk_golden_truth.txt')\n",
    "\n",
    "IMAGE_STR = 'data:image'\n",
    "\n",
    "train_labels_file = open(TRAIN_LABELS_PATH, 'r')\n",
    "train_label_dict = {}\n",
    "for line in train_labels_file:\n",
    "    xml_file, label = line.split(' ')\n",
    "    train_label_dict[xml_file] = label\n",
    "train_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = []\n",
    "y_train = []\n",
    "X_test_raw = []\n",
    "y_test = []\n",
    "\n",
    "train_entry_path_list = [TRAIN_POSITIVE_PATH, TRAIN_NEGATIVE_PATH]\n",
    "test_pos_entry_path_list = [TEST_POSITIVE_PATH]\n",
    "test_neg_entry_path_list = [TEST_NEGATIVE_PATH]\n",
    "\n",
    "read_entries(X=X_train_raw, y=y_train, path_list=train_entry_path_list, label_dict=train_label_dict)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_pos_entry_path_list, default_label=1)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_and_absolutizm_features(X_raw):\n",
    "    \n",
    "    fp_pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    \n",
    "    tp_pronouns = {'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "    \n",
    "    absolutisms = {'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "                   'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', \n",
    "                   'totally', 'whole', 'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "\n",
    "    fp_freq = []\n",
    "    tp_freq = []\n",
    "    absolutisms_freq = []\n",
    "    \n",
    "    for entry in X_raw:\n",
    "        sum_fp = 0\n",
    "        sum_tp = 0\n",
    "        sum_abs = 0\n",
    "        tokens = word_tokenize(entry)\n",
    "        for word in tokens:\n",
    "            if word in fp_pronouns:\n",
    "                sum_fp += 1\n",
    "            elif word in tp_pronouns:\n",
    "                sum_tp += 1\n",
    "            elif word in absolutisms:\n",
    "                sum_abs += 1\n",
    "        sum_fp = sum_fp / float(len(tokens))\n",
    "        sum_tp = sum_tp / float(len(tokens))\n",
    "        sum_abs = sum_abs / float(len(tokens))\n",
    "        fp_freq.append([sum_fp])\n",
    "        tp_freq.append([sum_tp])\n",
    "        absolutisms_freq.append([sum_abs])\n",
    "    \n",
    "    return (fp_freq, tp_freq, absolutisms_freq)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_and_absolutizm_features2(X_raw):\n",
    "    \n",
    "    fp_pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    \n",
    "    tp_pronouns = {'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "    \n",
    "    absolutisms = {'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "                   'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', \n",
    "                   'totally', 'whole', 'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "\n",
    "    fp_freq = []\n",
    "    tp_freq = []\n",
    "    absolutisms_freq = []\n",
    "    \n",
    "    for entry in X_raw:\n",
    "        sum_fp = 0\n",
    "        sum_tp = 0\n",
    "        sum_abs = 0\n",
    "        tokens = word_tokenize(entry)\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        tokens = [word for word in tokens if not word.isnumeric()]\n",
    "        fdist = FreqDist(tokens)\n",
    "        w, f = fdist.most_common(1)[0]\n",
    "        for word in tokens:\n",
    "            if word in fp_pronouns:\n",
    "                sum_fp += 1\n",
    "            elif word in tp_pronouns:\n",
    "                sum_tp += 1\n",
    "            elif word in absolutisms:\n",
    "                sum_abs += 1\n",
    "        sum_fp = sum_fp / float(f)\n",
    "        sum_tp = sum_tp / float(f)\n",
    "        sum_abs = sum_abs / float(f)\n",
    "        fp_freq.append([sum_fp])\n",
    "        tp_freq.append([sum_tp])\n",
    "        absolutisms_freq.append([sum_abs])\n",
    "    \n",
    "    return (fp_freq, tp_freq, absolutisms_freq)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_times_train = []\n",
    "X_times_test = []\n",
    "\n",
    "X_sentence_len_train = []\n",
    "X_sentence_len_test = []\n",
    "\n",
    "X_post_cnt_train = []\n",
    "X_post_cnt_test = []\n",
    "\n",
    "X_sentiment_train = []\n",
    "X_sentiment_test = []\n",
    "\n",
    "X_subjectivity_train = []\n",
    "X_subjectivity_test = []\n",
    "\n",
    "X_post_lengths_train = []\n",
    "X_post_lengths_test = []\n",
    "\n",
    "X_post_freq_train = []\n",
    "X_post_freq_test = []\n",
    "\n",
    "read_features(X_times=X_times_train, X_sentence_lengths=X_sentence_len_train, X_post_cnt=X_post_cnt_train,\n",
    "              X_sentiment=X_sentiment_train, X_subjectivity=X_subjectivity_train, X_post_lengths=X_post_lengths_train,\n",
    "              X_post_freq=X_post_freq_train, path_list=train_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_pos_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_neg_entry_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fp_pronouns_train, X_tp_pronouns_train, X_absolutisms_train = get_pronoun_and_absolutizm_features2(X_train_raw)\n",
    "X_fp_pronouns_test, X_tp_pronouns_test, X_absolutisms_test = get_pronoun_and_absolutizm_features2(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_features(X):\n",
    "    lexicon = Empath()\n",
    "    \n",
    "    relevant_lexical_categories = ['negative_emotion', 'positive_emotion', 'communication',\n",
    "                                    'violence', 'business', 'nervousness', 'body', 'pain',\n",
    "                                    'internet', 'work', 'shame', 'poor'\n",
    "                              ]\n",
    "    \n",
    "    relevant_lexical_categories2 = ['negative_emotion', 'positive_emotion',\n",
    "                                   'nervousness', 'love', 'shame', 'pain'\n",
    "                              ]\n",
    "    \n",
    "    relevant_lexical_categories3 = ['friends', 'positive_emotion', 'negative_emotion', \n",
    "                                   'nervousness', 'love', 'shame', 'pain', 'optimism', 'sadness',\n",
    "                                    'speaking'\n",
    "                              ]\n",
    "    \n",
    "    feature_mat = []\n",
    "    for text in X:\n",
    "        d = lexicon.analyze(text, categories=relevant_lexical_categories3, normalize=True)\n",
    "        feature_mat.append([d[key] for key in sorted(d.keys(), reverse=False)])\n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sem_feat_train = get_semantic_features(X_train_raw)\n",
    "X_sem_feat_test = get_semantic_features(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_times_train, open(\"./dumps/X_times_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_train, open(\"./dumps/X_sentence_len_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_train, open(\"./dumps/X_post_cnt_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_train, open(\"./dumps/X_sentiment_train.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_train, open(\"./dumps/X_subjectivity_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_train, open(\"./dumps/X_post_lengths_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_train, open(\"./dumps/X_post_freq_train.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_train, open(\"./dumps/X_fp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_train, open(\"./dumps/X_tp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_train, open(\"./dumps/X_absolutisms_train.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_times_test, open(\"./dumps/X_times_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_test, open(\"./dumps/X_sentence_len_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_test, open(\"./dumps/X_post_cnt_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_test, open(\"./dumps/X_sentiment_test.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_test, open(\"./dumps/X_subjectivity_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_test, open(\"./dumps/X_post_lengths_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_test, open(\"./dumps/X_post_freq_test.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_test, open(\"./dumps/X_fp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_test, open(\"./dumps/X_tp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_test, open(\"./dumps/X_absolutisms_test.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_sem_feat_train, open(\"./dumps/X_sem_feat_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sem_feat_test, open(\"./dumps/X_sem_feat_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use X list as input to NLTKPreprocessor class which outputs list of preprocessed, tokenized texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "X_train_prep = preprocessor.transform(X_train_raw, method=preprocess_method)\n",
    "X_test_prep = preprocessor.transform(X_test_raw, method=preprocess_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(X):\n",
    "    pos_tag_mat = []\n",
    "    for tokens in X:\n",
    "       # tag_dict = { 'CC': 0, 'DT': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, 'JJS': 0,\n",
    "       #             'NN': 0, 'NNP':0, 'NNS': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0,\n",
    "       #             'RBR': 0, 'RBS': 0, 'RP': 0, 'VB': 0, 'VBD': 0, 'VBG': 0,\n",
    "        #            'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        tag_dict = {'JJ': 0, 'JJS': 0,\n",
    "                    'NN': 0, 'NNS': 0, 'PRP': 0, 'PRP$': 0,\n",
    "                    'RBS': 0, 'VB': 0, 'VBD': 0,\n",
    "                    'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        \n",
    "        text_len = len(tokens)\n",
    "        tags = pos_tag(tokens)\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if tag in tag_dict.keys():\n",
    "                tag_dict[tag] += 1/text_len\n",
    "        \n",
    "        tag_freq = [tag_dict[key] for key in sorted(tag_dict.keys(), reverse=False)]\n",
    "        pos_tag_mat.append(tag_freq)\n",
    "    \n",
    "    return pos_tag_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_tags_train = get_pos_tags(X_train_prep)\n",
    "X_pos_tags_test = get_pos_tags(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_pos_tags_train, open(\"./dumps/X_pos_tags_train.p\", \"wb\" ))\n",
    "pickle.dump(X_pos_tags_test, open(\"./dumps/X_pos_tags_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_sizes(X):\n",
    "    unique_cnt_mat = []\n",
    "    for tokens in X:\n",
    "        unique_cnt_mat.append([len(set(tokens))])\n",
    "    \n",
    "    return unique_cnt_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lexicon_sizes_train = get_lexicon_sizes(X_train_prep)\n",
    "X_lexicon_sizes_test = get_lexicon_sizes(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_lexicon_sizes_train, open(\"./dumps/X_lexicon_sizes_train.p\", \"wb\" ))\n",
    "pickle.dump(X_lexicon_sizes_test, open(\"./dumps/X_lexicon_sizes_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tf-idf vectorizer for vector representation of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 1), min_df=30)\n",
    "X_train = vect.fit_transform(X_train_prep, y_train)\n",
    "X_test = vect.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vect.get_feature_names())\n",
    "#print(X_train.getnnz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and adding features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = StandardScaler()\n",
    "#max_abs_scaler = MaxAbsScaler()\n",
    "#max_abs_scaler = MinMaxScaler()\n",
    "X_sentence_len_train_scaled = max_abs_scaler.fit_transform(X_sentence_len_train)\n",
    "X_sentence_len_test_scaled = max_abs_scaler.transform(X_sentence_len_test)\n",
    "\n",
    "X_sem_feat_train_scaled = max_abs_scaler.fit_transform(X_sem_feat_train)\n",
    "X_sem_feat_test_scaled = max_abs_scaler.transform(X_sem_feat_test)\n",
    "\n",
    "X_post_cnt_train_scaled = max_abs_scaler.fit_transform(X_post_cnt_train)\n",
    "X_post_cnt_test_scaled = max_abs_scaler.transform(X_post_cnt_test)\n",
    "\n",
    "X_sentiment_train_scaled = max_abs_scaler.fit_transform(X_sentiment_train)\n",
    "X_sentiment_test_scaled = max_abs_scaler.transform(X_sentiment_test)\n",
    "\n",
    "X_subjectivity_train_scaled = max_abs_scaler.fit_transform(X_subjectivity_train)\n",
    "X_subjectivity_test_scaled = max_abs_scaler.transform(X_subjectivity_test)\n",
    "\n",
    "X_fp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_fp_pronouns_train)\n",
    "X_fp_pronouns_test_scaled = max_abs_scaler.transform(X_fp_pronouns_test)\n",
    "\n",
    "X_tp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_tp_pronouns_train)\n",
    "X_tp_pronouns_test_scaled = max_abs_scaler.transform(X_tp_pronouns_test)\n",
    "\n",
    "X_absolutisms_train_scaled = max_abs_scaler.fit_transform(X_absolutisms_train)\n",
    "X_absolutisms_test_scaled = max_abs_scaler.transform(X_absolutisms_test)\n",
    "\n",
    "X_pos_tags_train_scaled = max_abs_scaler.fit_transform(X_pos_tags_train)\n",
    "X_pos_tags_test_scaled = max_abs_scaler.transform(X_pos_tags_test)\n",
    "\n",
    "X_lexicon_sizes_train_scaled = max_abs_scaler.fit_transform(X_lexicon_sizes_train)\n",
    "X_lexicon_sizes_test_scaled = max_abs_scaler.transform(X_lexicon_sizes_test)\n",
    "\n",
    "X_post_lengths_train_scaled = max_abs_scaler.fit_transform(X_post_lengths_train)\n",
    "X_post_lengths_test_scaled = max_abs_scaler.transform(X_post_lengths_test)\n",
    "\n",
    "X_post_freq_train_scaled = max_abs_scaler.fit_transform(X_post_freq_train)\n",
    "X_post_freq_test_scaled = max_abs_scaler.transform(X_post_freq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and evaluating models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find allsubsets of a set (needed to find all subsets of set of new feature for CV)\n",
    "allsubsets = lambda n: list(chain(*[combinations(range(n), ni) for ni in range(n+1)]))\n",
    "\n",
    "# r - return only subsets of r size (reduction of search space) + empty set; if None, return all subsets\n",
    "def get_subsets(n, r=None):\n",
    "    if r==None:\n",
    "        return allsubsets(n)\n",
    "    else:\n",
    "        combs = list(combinations(range(n), r))\n",
    "        return combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_weight(w):\n",
    "    return 1.0 / (1.0 + w)\n",
    "\n",
    "def minority_weight(w):\n",
    "    return w / (1.0 + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(get_subsets(12, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for k-fold cross-validation of a model\n",
    "#Performs grid search on C parameter, subsets of new features and k-best token features by chi2 stat\n",
    "#For model_name parameter use 'SVM' - SVM or 'LR' - LogisticRegression\n",
    "\n",
    "#EXAMPLE: see example below baseline for CV of SVM model\n",
    "def cross_validate_model(model_name, X_train, y_train, C_list, k_list, new_features, k_folds, subset_size=None,\n",
    "                        regularization='l2'):\n",
    "    feature_subsets = get_subsets(len(new_features), subset_size)\n",
    "    best_C = 0\n",
    "    best_k = 0\n",
    "    best_feature_set = {}\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        \n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        for c in C_list:\n",
    "            \n",
    "            if model_name == 'SVM':\n",
    "                model = LinearSVC(class_weight='balanced', C=c, penalty=regularization) \n",
    "            elif model_name == 'LR':\n",
    "                model = LogisticRegression(class_weight='balanced', C=c, penalty=regularization)\n",
    "            \n",
    "            for subset in feature_subsets:\n",
    "                X_new = X_kbest_train\n",
    "                for i in subset:\n",
    "                    X_new = hstack([X_new, new_features[i]])\n",
    "                scores = cross_val_score(model, X_new, y_train, cv=k_folds, scoring=scorer)\n",
    "                mean = scores.mean()\n",
    "                if mean > best_score:\n",
    "                    best_score = mean\n",
    "                    best_C = c\n",
    "                    best_k = k_features\n",
    "                    best_feature_set = subset\n",
    "         \n",
    "    return (best_score, best_k, best_C, best_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cross_validate_model(model_name, X_train, y_train, C_list, k_list, k_folds, feature, regularization='l2'):\n",
    "    best_C = 0\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])    \n",
    "    best_k = 0\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        X_kbest_train = hstack([X_kbest_train, feature])\n",
    "\n",
    "        for c in C_list:\n",
    "                if model_name == 'SVM':\n",
    "                    model = LinearSVC(class_weight='balanced', C=c, penalty=regularization) \n",
    "                elif model_name == 'LR':\n",
    "                    model = LogisticRegression(class_weight='balanced', C=c, penalty=regularization)\n",
    "\n",
    "                scores = cross_val_score(model, X_kbest_train, y_train, cv=k_folds, scoring=scorer)\n",
    "                mean = scores.mean()\n",
    "                if mean > best_score:\n",
    "                        best_score = mean\n",
    "                        best_C = c\n",
    "                        best_k = k_features\n",
    "\n",
    "    return (best_score, best_C, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline(model_name, X_train, y_train, C_list, k_list, k_folds, regularization='l2'):\n",
    "    best_C = 0\n",
    "    best_score = -1\n",
    "    best_k = 0\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        \n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        for c in C_list:\n",
    "            if model_name == 'SVM':\n",
    "                model = LinearSVC(class_weight=\"balanced\", C=c, penalty=regularization) \n",
    "            elif model_name == 'LR':\n",
    "                model = LogisticRegression(class_weight=\"balanced\", C=c, penalty=regularization)\n",
    "\n",
    "            scores = cross_val_score(model, X_kbest_train, y_train, cv=k_folds, scoring=scorer)\n",
    "            mean = scores.mean()\n",
    "            if mean > best_score:\n",
    "                    best_score = mean\n",
    "                    best_C = c\n",
    "                    best_k = k_features\n",
    "         \n",
    "    return (best_score, best_k, best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_new_features(model_name, X_train, y_train, C_list, k_folds, regularization='l2'):\n",
    "    best_C = 0\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "        \n",
    "    for c in C_list:\n",
    "        if model_name == 'SVM':\n",
    "            model = LinearSVC(class_weight=\"balanced\", C=c, penalty=regularization) \n",
    "        elif model_name == 'LR':\n",
    "            model = LogisticRegression(class_weight=\"balanced\", C=c, penalty=regularization)\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=k_folds, scoring=scorer)\n",
    "        mean = scores.mean()\n",
    "        if mean > best_score:\n",
    "                best_score = mean\n",
    "                best_C = c\n",
    "         \n",
    "    return (best_score, best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6671353496353496\n",
      "Best C: 16.0\n",
      "Best k: 1500\n"
     ]
    }
   ],
   "source": [
    "#CV for Logistic Regression Baseline\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "k_range = [1000, 1500, 'all']\n",
    "\n",
    "best_score, best_k, best_C = cross_validate_baseline('LR', X_train, y_train, C_range, k_range, 5, 'l1')\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6879221432162609\n",
      "Best C: 8.0\n",
      "Best k: 1500\n"
     ]
    }
   ],
   "source": [
    "#CV for SVM baseline\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "k_range = [1000, 1500, 'all']\n",
    "\n",
    "best_score, best_k, best_C = cross_validate_baseline('SVM', X_train, y_train, C_range, k_range, 5)\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: BASELINE Logistic Regression L1\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9517    0.8949    0.9224       352\n",
      "          1     0.5067    0.7037    0.5891        54\n",
      "\n",
      "avg / total     0.8925    0.8695    0.8781       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: BASELINE Logistic Regression L1\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', penalty='l1', C=16.0)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "model.fit(X_kbest_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_kbest_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: BASELINE SVM\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9547    0.8977    0.9253       352\n",
      "          1     0.5200    0.7222    0.6047        54\n",
      "\n",
      "avg / total     0.8969    0.8744    0.8827       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: BASELINE SVM\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced', C=8.0)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "model.fit(X_kbest_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_kbest_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier for baselines (SVM + LR)\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9452    0.9318    0.9385       352\n",
      "          1     0.5932    0.6481    0.6195        54\n",
      "\n",
      "avg / total     0.8984    0.8941    0.8961       406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrija/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(\"Voting classifier for baselines (SVM + LR)\")\n",
    "\n",
    "model1 = LogisticRegression(class_weight='balanced', C=16.0, penalty='l1')\n",
    "model2 = SVC(class_weight='balanced', kernel='linear', C=8.0, probability=True)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('svm', model2)], voting='soft')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "model.fit(X_kbest_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_kbest_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_training = [X_sentence_len_train_scaled, X_sem_feat_train_scaled[:,:3],\n",
    "                      X_sentiment_train_scaled, X_subjectivity_train_scaled, \n",
    "                      X_fp_pronouns_train_scaled, X_tp_pronouns_train_scaled, X_absolutisms_train_scaled, \n",
    "                      X_lexicon_sizes_train_scaled, X_post_lengths_train_scaled, \n",
    "                      X_post_freq_train_scaled]\n",
    "\n",
    "reduced_testing = [X_sentence_len_test_scaled, X_sem_feat_test_scaled[:,:3],\n",
    "                    X_sentiment_test_scaled, X_subjectivity_test_scaled, \n",
    "                      X_fp_pronouns_test_scaled, X_tp_pronouns_test_scaled, X_absolutisms_test_scaled, \n",
    "                      X_lexicon_sizes_test_scaled, X_post_lengths_test_scaled, \n",
    "                      X_post_freq_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 6 (0.164981)\n",
      "2. feature 8 (0.099740)\n",
      "3. feature 2 (0.090884)\n",
      "4. feature 3 (0.090752)\n",
      "5. feature 10 (0.079123)\n",
      "6. feature 5 (0.078314)\n",
      "7. feature 1 (0.073578)\n",
      "8. feature 11 (0.073574)\n",
      "9. feature 7 (0.073513)\n",
      "10. feature 9 (0.064031)\n",
      "11. feature 0 (0.057355)\n",
      "12. feature 4 (0.054154)\n",
      "(486, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafdbfad278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=500, random_state=0, class_weight='balanced')\n",
    "X = np.hstack(reduced_training)\n",
    "forest.fit(X, y_train)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "selector = SelectFromModel(forest, prefit=True)\n",
    "X_train_new = selector.transform(np.hstack(reduced_training))\n",
    "X_test_new = selector.transform(np.hstack(reduced_testing))\n",
    "print(X_train_new.shape)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6750180375180376\n",
      "Best C: 22\n",
      "Best k: 1500\n"
     ]
    }
   ],
   "source": [
    "#Individual feature CV\n",
    "#C_range = 2. ** np.arange(-1, 8)\n",
    "C_range = [1, 2, 4, 8, 9, 10, 12, 14, 16, 18, 20, 21, 22, 23, 32, 64]\n",
    "k_range = [1500]\n",
    "best_score, best_C, best_k = feature_cross_validate_model('LR', X_train, y_train, C_range, k_range, 5, \n",
    "                                                  X_train_new, regularization='l1')\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: Logistic Regression classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9521    0.9034    0.9271       352\n",
      "          1     0.5278    0.7037    0.6032        54\n",
      "\n",
      "avg / total     0.8957    0.8768    0.8840       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: Logistic Regression classifier\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', C=22.0, penalty='l1')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = hstack([X_kbest_train, X_train_new])  \n",
    "X_final_test = hstack([X_kbest_test, X_test_new])\n",
    "\n",
    "#X_final_train = np.hstack(final_training)\n",
    "#X_final_test = np.hstack(final_testing)\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: SVM classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9550    0.9034    0.9285       352\n",
      "          1     0.5342    0.7222    0.6142        54\n",
      "\n",
      "avg / total     0.8990    0.8793    0.8867       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: SVM classifier\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced', C=10.0)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = hstack([X_kbest_train, X_train_new])  \n",
    "X_final_test = hstack([X_kbest_test, X_test_new])\n",
    "#X_final_train = np.hstack(final_training)\n",
    "#X_final_test = np.hstack(final_testing)\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier for baselines (SVM + LR) with additional features\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9429    0.9375    0.9402       352\n",
      "          1     0.6071    0.6296    0.6182        54\n",
      "\n",
      "avg / total     0.8982    0.8966    0.8973       406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrija/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(\"Voting classifier for baselines (SVM + LR) with additional features\")\n",
    "\n",
    "model1 = LogisticRegression(class_weight='balanced', C=22.0, penalty='l1')\n",
    "model2 = SVC(class_weight='balanced', kernel='linear', C=10.0, probability=True)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('svm', model2)], voting='soft')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = hstack([X_kbest_train, X_train_new])  \n",
    "X_final_test = hstack([X_kbest_test, X_test_new])\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the complete model on whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vstack((X_train_2, X_test_2))\n",
    "y = y_train + y_test\n",
    "\n",
    "model_complete = LinearSVC(class_weight='balanced')\n",
    "model_complete.fit(X, y)\n",
    "\n",
    "print(\"Complete model fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(show_most_informative_features(vect, model_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some baseline classifier testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 2), min_df=20)\n",
    "X_train_3 = vect2.fit_transform(X_train_prep, y_train)\n",
    "X_test_3 = vect2.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: BernoulliNB classifier\")\n",
    "\n",
    "model2 = BernoulliNB()\n",
    "model2.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model2.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: MultinomialNB classifier\")\n",
    "\n",
    "model3 = MultinomialNB()\n",
    "model3.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model3.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: DecisionTree classifier\")\n",
    "\n",
    "model4 = DecisionTreeClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: RandomForest classifier\")\n",
    "\n",
    "model4 = RandomForestClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Empath testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "relevant_lexical_categories1 = ['help', 'medical_emergency', 'hate', 'health', 'suffering', \n",
    "                               'kill', 'fear', 'death', 'violence', 'love',\n",
    "                               'anonymity', 'injury', 'appearance', 'sadness',\n",
    "                               'emotional', 'ugliness', 'shame', 'torment',\n",
    "                               'pain', 'negative_emotion', 'positive_emotion', 'friends',\n",
    "                               'alcohol', 'nervousness', 'optimism', 'body', 'contentment'\n",
    "                               'cold', 'school', 'communication', 'work', 'sleep', 'play'\n",
    "                               'trust', 'social_media', 'sexual'\n",
    "                              ]\n",
    "\n",
    "relevant_lexical_categories = ['negative_emotion', 'speaking', 'positive_emotion', 'communication',\n",
    "                               'friends', 'children', 'optimism', 'violence', 'pain', 'family',\n",
    "                               'trust', 'love', 'party', 'business', 'home', 'shame', 'listen',\n",
    "                               'giving', 'body', 'suffering', 'work', 'nervousness', 'strength',\n",
    "                               'hearing', 'health', 'traveling', 'wedding', 'childish', 'hate',\n",
    "                               'social_media', 'sadness', 'school'\n",
    "                              ]\n",
    "\n",
    "x_senti1 = []\n",
    "y_senti1 = []\n",
    "read_entries(X=x_senti1, y=y_senti1, path_list=test_pos_entry_path_list, default_label=1)\n",
    "\n",
    "x_senti2 = []\n",
    "y_senti2 = []\n",
    "read_entries(X=x_senti2, y=y_senti2, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dict1 = {}\n",
    "len1 = len(x_senti1)\n",
    "for i in x_senti1:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict1 = { k: d.get(k, 0)/len1 + avg_dict1.get(k, 0) for k in set(d) | set(avg_dict1) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict1.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dict2 = {}\n",
    "len2 = len(x_senti2)\n",
    "for i in x_senti2:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict2 = { k: d.get(k, 0)/len2 + avg_dict2.get(k, 0) for k in set(d) | set(avg_dict2) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict2.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(list(lexicon.analyze(x_senti2[2], normalize=True).values()))\n",
    "d = lexicon.analyze(x_senti2[2], normalize=True)\n",
    "for w in sorted(d.keys(), reverse=False):\n",
    "    print(w, d[w])\n",
    "\n",
    "result = [d[key] for key in sorted(d.keys(), reverse=False)]\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen = 'It is not exactly a big deal, but a huge sigh of relief to get confirmation that the movie is on the right track. I love life.'\n",
    "print(sent_tokenize(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_i = []\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i] != y_pred[i]):\n",
    "        user_i.append(i)\n",
    "\n",
    "entry_lists = []\n",
    "path_list = test_pos_entry_path_list + test_neg_entry_path_list\n",
    "for path in path_list:\n",
    "    entry_lists.append(os.scandir(path))\n",
    "    \n",
    "users = []\n",
    "\n",
    "for list_of_entries in entry_lists:\n",
    "    for entry in list_of_entries:\n",
    "        root = etree.parse(entry.path).getroot()\n",
    "        user_id = root[0].text\n",
    "        users.append(user_id)\n",
    "\n",
    "for i in user_i:\n",
    "    print(users[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lexicon.analyze(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = ['I love beinggg retarded']\n",
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "example2 = preprocessor.transform(example, method=preprocess_method)\n",
    "\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
