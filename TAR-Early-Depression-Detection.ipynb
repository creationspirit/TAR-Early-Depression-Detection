{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrija/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from itertools import combinations, chain\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from lxml import etree\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from scipy.sparse import vstack, hstack, coo_matrix\n",
    "\n",
    "from empath import Empath\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle up all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\n",
    "X_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\n",
    "X_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\n",
    "X_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\n",
    "X_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\n",
    "\n",
    "X_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\n",
    "X_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\n",
    "X_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\n",
    "X_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\n",
    "X_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\n",
    "\n",
    "X_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\n",
    "X_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\n",
    "\n",
    "X_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\n",
    "X_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Corpus preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_diff = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'he', 'him', 'his', 'himself', \n",
    "           'she', 'her', 'hers', 'herself', 'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', \n",
    "           'themselves', 'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "           'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', 'totally', 'whole',\n",
    "           'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.stopwords.difference_update(sw_diff)\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X, method='lem'):\n",
    "        return [\n",
    "            list(self.tokenize(doc, method)) for doc in X\n",
    "        ]   \n",
    "\n",
    "    def tokenize(self, document, method='lem'):\n",
    "        if(method == 'lem'):\n",
    "            # Break the document into sentences\n",
    "            for sent in sent_tokenize(document):\n",
    "                # Break the sentence into part of speech tagged tokens\n",
    "                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                    # Apply preprocessing to the token\n",
    "                    token = self.process_token(token)\n",
    "                    if not self.is_valid_token(token):\n",
    "                        continue\n",
    "                        \n",
    "                    # Lemmatize the token and yield\n",
    "                    lemma = self.lemmatize(token, tag)\n",
    "                    yield lemma\n",
    "                    \n",
    "        elif(method == 'stem'):\n",
    "            # Break the document into tokens\n",
    "            for token in wordpunct_tokenize(document):\n",
    "                # Apply preprocessing to the token\n",
    "                token = self.process_token(token)\n",
    "                if not self.is_valid_token(token):\n",
    "                    continue\n",
    "                \n",
    "                stem = self.stem(token)\n",
    "                yield stem\n",
    "        else:\n",
    "            raise ValueError('Unknown method type.')\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def stem(self, token):\n",
    "        return self.stemmer.stem(token)\n",
    "    \n",
    "    def process_token(self, token):\n",
    "        token = token.lower() if self.lower else token\n",
    "        token = token.strip() if self.strip else tcharoken\n",
    "        token = token.strip('_') if self.strip else token\n",
    "        token = token.strip('*') if self.strip else token\n",
    "        return token\n",
    "    \n",
    "    def is_valid_token(self, token):\n",
    "        # If stopword, token is invalid\n",
    "        if token in self.stopwords:\n",
    "            return False\n",
    "\n",
    "        # If punctuation, token is invalid\n",
    "        if all(char in self.punct for char in token):\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code loads data corpus from multiple files into lists X (texts) and y(labels) with one entry per user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entries(X, y, path_list, label_dict=None, default_label=0):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "    \n",
    "    IMAGE_STR = 'data:image'\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "        \n",
    "            user_text = ''\n",
    "            for post in root.findall('.//TITLE') + root.findall('.//TEXT'):\n",
    "                post = post.text.strip().strip()\n",
    "                if post != '':\n",
    "                    if IMAGE_STR in post:\n",
    "                        continue\n",
    "                    post = re.sub(r\"http\\S+\", \" \", post)\n",
    "                    post = re.sub(r\"\\d+\", \" \", post)\n",
    "                    post = re.sub(u\"\\xa0\", \" \", post)\n",
    "                    post = re.sub(u\"\\\\p{P}+\", \" \", post)\n",
    "                    user_text += ' ' + post.lower()\n",
    "            \n",
    "            X.append(user_text)\n",
    "            label = int(label_dict[user_id]) if label_dict else default_label\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility methods for extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sentence_length(sentences):\n",
    "    sum = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(' ', '')\n",
    "        sum += len(sentence)\n",
    "    return sum / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_and_subjectivity(sentences):\n",
    "    sum_sentiment = 0\n",
    "    sum_subjectivity = 0\n",
    "    if len(sentences) > 0:\n",
    "        for sentence in sentences:\n",
    "            tb = TextBlob(sentence)\n",
    "            sum_sentiment += tb.sentiment.polarity\n",
    "            sum_subjectivity += tb.sentiment.subjectivity\n",
    "        sum_sentiment = sum_sentiment / float(len(sentences))\n",
    "        sum_subjectivity = sum_subjectivity / float(len(sentences))\n",
    "        return (sum_sentiment, sum_subjectivity)\n",
    "    else:\n",
    "        return (0.0, 0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(X_times, X_sentence_lengths, X_post_cnt, X_sentiment,\n",
    "                  X_subjectivity, X_post_lengths, X_post_freq, path_list):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "        \n",
    "    IMAGE_STR = 'data:image'\n",
    "    datetime_pattern = '%Y-%m-%d %H:%M:%S'\n",
    "    date_end = None\n",
    "    date_start = None\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "            \n",
    "            sentences = []\n",
    "            post_lengths = []\n",
    "            post_cnt = 0\n",
    "            for post in root.findall('.//TEXT'):\n",
    "                post_cnt += 1\n",
    "                post = post.text.strip()\n",
    "                if post != '':\n",
    "                    sentences.extend(sent_tokenize(post))\n",
    "                    post_lengths.append(len(post))\n",
    "                else:\n",
    "                    post_lengths.append(0)\n",
    "            \n",
    "            avg_sentiment, avg_subjectivity = get_sentiment_and_subjectivity(sentences)\n",
    "            avg_sentence_length = get_avg_sentence_length(sentences)\n",
    "            avg_post_length = np.mean(post_lengths)\n",
    "            \n",
    "            sum_hours = 0\n",
    "            for date in root.findall('.//DATE'):\n",
    "                date = date.text.strip()\n",
    "                if date != '':                    \n",
    "                    if not date_end:\n",
    "                        date_end = datetime.strptime(date, datetime_pattern)\n",
    "                    date_start = datetime.strptime(date, datetime_pattern)\n",
    "                    \n",
    "                    m = re.match(r'\\d{4}-\\d{2}-\\d{2} (\\d{2}).*', date)\n",
    "                    hour = int(m.group(1))\n",
    "                    sum_hours += hour\n",
    "            \n",
    "            post_span_minutes = (date_end - date_start).total_seconds()/60\n",
    "            post_freq = post_span_minutes / post_cnt\n",
    "            \n",
    "            time = [0] * 8\n",
    "            avg_hour = sum_hours / post_cnt\n",
    "            index = int(avg_hour // 3)\n",
    "            time[index] = 1\n",
    "            \n",
    "            X_post_cnt.append([post_cnt])\n",
    "            X_sentence_lengths.append([avg_sentence_length])\n",
    "            X_times.append(time)\n",
    "            X_sentiment.append([avg_sentiment])\n",
    "            X_subjectivity.append([avg_subjectivity])\n",
    "            X_post_lengths.append([avg_post_length])\n",
    "            X_post_freq.append([post_freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "TRAIN_PATH = os.path.join(cwd, \"reddit-training-ready-to-share\")\n",
    "TEST_PATH = os.path.join(cwd, \"reddit-test-data-ready-to-share\")\n",
    "\n",
    "TRAIN_POSITIVE_PATH = os.path.join(TRAIN_PATH, \"positive_examples_anonymous\")\n",
    "TRAIN_NEGATIVE_PATH = os.path.join(TRAIN_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TEST_POSITIVE_PATH = os.path.join(TEST_PATH, \"positive_examples_anonymous\")\n",
    "TEST_NEGATIVE_PATH = os.path.join(TEST_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TRAIN_LABELS_PATH = os.path.join(cwd, 'risk_golden_truth.txt')\n",
    "\n",
    "IMAGE_STR = 'data:image'\n",
    "\n",
    "train_labels_file = open(TRAIN_LABELS_PATH, 'r')\n",
    "train_label_dict = {}\n",
    "for line in train_labels_file:\n",
    "    xml_file, label = line.split(' ')\n",
    "    train_label_dict[xml_file] = label\n",
    "train_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = []\n",
    "y_train = []\n",
    "X_test_raw = []\n",
    "y_test = []\n",
    "\n",
    "train_entry_path_list = [TRAIN_POSITIVE_PATH, TRAIN_NEGATIVE_PATH]\n",
    "test_pos_entry_path_list = [TEST_POSITIVE_PATH]\n",
    "test_neg_entry_path_list = [TEST_NEGATIVE_PATH]\n",
    "\n",
    "read_entries(X=X_train_raw, y=y_train, path_list=train_entry_path_list, label_dict=train_label_dict)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_pos_entry_path_list, default_label=1)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_and_absolutizm_features(X_raw):\n",
    "    \n",
    "    fp_pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    \n",
    "    tp_pronouns = {'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "    \n",
    "    absolutisms = {'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "                   'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', \n",
    "                   'totally', 'whole', 'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "\n",
    "    fp_freq = []\n",
    "    tp_freq = []\n",
    "    absolutisms_freq = []\n",
    "    \n",
    "    for entry in X_raw:\n",
    "        sum_fp = 0\n",
    "        sum_tp = 0\n",
    "        sum_abs = 0\n",
    "        tokens = word_tokenize(entry)\n",
    "        for word in tokens:\n",
    "            if word in fp_pronouns:\n",
    "                sum_fp += 1\n",
    "            elif word in tp_pronouns:\n",
    "                sum_tp += 1\n",
    "            elif word in absolutisms:\n",
    "                sum_abs += 1\n",
    "        sum_fp = sum_fp / float(len(tokens))\n",
    "        sum_tp = sum_tp / float(len(tokens))\n",
    "        sum_abs = sum_abs / float(len(tokens))\n",
    "        fp_freq.append([sum_fp])\n",
    "        tp_freq.append([sum_tp])\n",
    "        absolutisms_freq.append([sum_abs])\n",
    "    \n",
    "    return (fp_freq, tp_freq, absolutisms_freq)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_times_train = []\n",
    "X_times_test = []\n",
    "\n",
    "X_sentence_len_train = []\n",
    "X_sentence_len_test = []\n",
    "\n",
    "X_post_cnt_train = []\n",
    "X_post_cnt_test = []\n",
    "\n",
    "X_sentiment_train = []\n",
    "X_sentiment_test = []\n",
    "\n",
    "X_subjectivity_train = []\n",
    "X_subjectivity_test = []\n",
    "\n",
    "X_post_lengths_train = []\n",
    "X_post_lengths_test = []\n",
    "\n",
    "X_post_freq_train = []\n",
    "X_post_freq_test = []\n",
    "\n",
    "read_features(X_times=X_times_train, X_sentence_lengths=X_sentence_len_train, X_post_cnt=X_post_cnt_train,\n",
    "              X_sentiment=X_sentiment_train, X_subjectivity=X_subjectivity_train, X_post_lengths=X_post_lengths_train,\n",
    "              X_post_freq=X_post_freq_train, path_list=train_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_pos_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_neg_entry_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fp_pronouns_train, X_tp_pronouns_train, X_absolutisms_train = get_pronoun_and_absolutizm_features(X_train_raw)\n",
    "X_fp_pronouns_test, X_tp_pronouns_test, X_absolutisms_test = get_pronoun_and_absolutizm_features(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_features(X):\n",
    "    lexicon = Empath()\n",
    "    \n",
    "    relevant_lexical_categories = ['negative_emotion', 'positive_emotion', 'communication',\n",
    "                                    'violence', 'business', 'nervousness', 'body', 'pain',\n",
    "                                    'internet', 'work', 'shame', 'poor'\n",
    "                              ]\n",
    "    \n",
    "    relevant_lexical_categories2 = ['negative_emotion', 'positive_emotion',\n",
    "                                   'nervousness', 'love', 'shame', 'pain'\n",
    "                              ]\n",
    "    \n",
    "    feature_mat = []\n",
    "    for text in X:\n",
    "        d = lexicon.analyze(text, categories=relevant_lexical_categories2, normalize=True)\n",
    "        feature_mat.append([d[key] for key in sorted(d.keys(), reverse=False)])\n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sem_feat_train = get_semantic_features(X_train_raw)\n",
    "X_sem_feat_test = get_semantic_features(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_times_train, open(\"./dumps/X_times_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_train, open(\"./dumps/X_sentence_len_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_train, open(\"./dumps/X_post_cnt_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_train, open(\"./dumps/X_sentiment_train.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_train, open(\"./dumps/X_subjectivity_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_train, open(\"./dumps/X_post_lengths_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_train, open(\"./dumps/X_post_freq_train.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_train, open(\"./dumps/X_fp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_train, open(\"./dumps/X_tp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_train, open(\"./dumps/X_absolutisms_train.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_times_test, open(\"./dumps/X_times_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_test, open(\"./dumps/X_sentence_len_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_test, open(\"./dumps/X_post_cnt_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_test, open(\"./dumps/X_sentiment_test.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_test, open(\"./dumps/X_subjectivity_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_test, open(\"./dumps/X_post_lengths_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_test, open(\"./dumps/X_post_freq_test.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_test, open(\"./dumps/X_fp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_test, open(\"./dumps/X_tp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_test, open(\"./dumps/X_absolutisms_test.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_sem_feat_train, open(\"./dumps/X_sem_feat_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sem_feat_test, open(\"./dumps/X_sem_feat_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use X list as input to NLTKPreprocessor class which outputs list of preprocessed, tokenized texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "X_train_prep = preprocessor.transform(X_train_raw, method=preprocess_method)\n",
    "X_test_prep = preprocessor.transform(X_test_raw, method=preprocess_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(X):\n",
    "    pos_tag_mat = []\n",
    "    for tokens in X:\n",
    "        tag_dict = { 'CC': 0, 'DT': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, 'JJS': 0,\n",
    "                    'NN': 0, 'NNP':0, 'NNS': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0,\n",
    "                    'RBR': 0, 'RBS': 0, 'RP': 0, 'VB': 0, 'VBD': 0, 'VBG': 0,\n",
    "                    'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        \n",
    "        text_len = len(tokens)\n",
    "        tags = pos_tag(tokens)\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if tag in tag_dict.keys():\n",
    "                tag_dict[tag] += 1/text_len\n",
    "        \n",
    "        tag_freq = [tag_dict[key] for key in sorted(tag_dict.keys(), reverse=False)]\n",
    "        pos_tag_mat.append(tag_freq)\n",
    "    \n",
    "    return pos_tag_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_tags_train = get_pos_tags(X_train_prep)\n",
    "X_pos_tags_test = get_pos_tags(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_pos_tags_train, open(\"./dumps/X_pos_tags_train.p\", \"wb\" ))\n",
    "pickle.dump(X_pos_tags_test, open(\"./dumps/X_pos_tags_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_sizes(X):\n",
    "    unique_cnt_mat = []\n",
    "    for tokens in X:\n",
    "        unique_cnt_mat.append([len(set(tokens))])\n",
    "    \n",
    "    return unique_cnt_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lexicon_sizes_train = get_lexicon_sizes(X_train_prep)\n",
    "X_lexicon_sizes_test = get_lexicon_sizes(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_lexicon_sizes_train, open(\"./dumps/X_lexicon_sizes_train.p\", \"wb\" ))\n",
    "pickle.dump(X_lexicon_sizes_test, open(\"./dumps/X_lexicon_sizes_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tf-idf vectorizer for vector representation of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 1), min_df=30)\n",
    "X_train = vect.fit_transform(X_train_prep, y_train)\n",
    "X_test = vect.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762629\n"
     ]
    }
   ],
   "source": [
    "#print(vect.get_feature_names())\n",
    "#print(X_train.getnnz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and adding features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = MaxAbsScaler()\n",
    "X_sentence_len_train_scaled = max_abs_scaler.fit_transform(X_sentence_len_train)\n",
    "X_sentence_len_test_scaled = max_abs_scaler.transform(X_sentence_len_test)\n",
    "\n",
    "X_sem_feat_train_scaled = max_abs_scaler.fit_transform(X_sem_feat_train)\n",
    "X_sem_feat_test_scaled = max_abs_scaler.transform(X_sem_feat_test)\n",
    "\n",
    "X_post_cnt_train_scaled = max_abs_scaler.fit_transform(X_post_cnt_train)\n",
    "X_post_cnt_test_scaled = max_abs_scaler.transform(X_post_cnt_test)\n",
    "\n",
    "X_sentiment_train_scaled = max_abs_scaler.fit_transform(X_sentiment_train)\n",
    "X_sentiment_test_scaled = max_abs_scaler.transform(X_sentiment_test)\n",
    "\n",
    "X_subjectivity_train_scaled = max_abs_scaler.fit_transform(X_subjectivity_train)\n",
    "X_subjectivity_test_scaled = max_abs_scaler.transform(X_subjectivity_test)\n",
    "\n",
    "X_fp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_fp_pronouns_train)\n",
    "X_fp_pronouns_test_scaled = max_abs_scaler.transform(X_fp_pronouns_test)\n",
    "\n",
    "X_tp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_tp_pronouns_train)\n",
    "X_tp_pronouns_test_scaled = max_abs_scaler.transform(X_tp_pronouns_test)\n",
    "\n",
    "X_absolutisms_train_scaled = max_abs_scaler.fit_transform(X_absolutisms_train)\n",
    "X_absolutisms_test_scaled = max_abs_scaler.transform(X_absolutisms_test)\n",
    "\n",
    "X_pos_tags_train_scaled = max_abs_scaler.fit_transform(X_pos_tags_train)\n",
    "X_pos_tags_test_scaled = max_abs_scaler.fit_transform(X_pos_tags_test)\n",
    "\n",
    "X_lexicon_sizes_train_scaled = max_abs_scaler.fit_transform(X_lexicon_sizes_train)\n",
    "X_lexicon_sizes_test_scaled = max_abs_scaler.fit_transform(X_lexicon_sizes_test)\n",
    "\n",
    "X_post_lengths_train_scaled = max_abs_scaler.fit_transform(X_post_lengths_train)\n",
    "X_post_lengths_test_scaled = max_abs_scaler.fit_transform(X_post_lengths_test)\n",
    "\n",
    "X_post_freq_train_scaled = max_abs_scaler.fit_transform(X_post_freq_train)\n",
    "X_post_freq_test_scaled = max_abs_scaler.fit_transform(X_post_freq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and evaluating models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find allsubsets of a set (needed to find all subsets of set of new feature for CV)\n",
    "allsubsets = lambda n: list(chain(*[combinations(range(n), ni) for ni in range(n+1)]))\n",
    "\n",
    "# r - return only subsets of r size (reduction of search space) + empty set; if None, return all subsets\n",
    "def get_subsets(n, r=None):\n",
    "    if r==None:\n",
    "        return allsubsets(n)\n",
    "    else:\n",
    "        combs = list(combinations(range(n), r))\n",
    "        combs.append(())\n",
    "        return combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "print(len(get_subsets(10, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for k-fold cross-validation of a model\n",
    "#Performs grid search on C parameter, subsets of new features and k-best token features by chi2 stat\n",
    "#For model_name parameter use 'SVM' - SVM or 'LR' - LogisticRegression\n",
    "\n",
    "#EXAMPLE: see example below baseline for CV of SVM model\n",
    "def cross_validate_model(model_name, X_train, y_train, C_list, k_list, new_features, k_folds, subset_size=None,\n",
    "                        regularization='l2'):\n",
    "    feature_subsets = get_subsets(len(new_features), subset_size)\n",
    "    best_C = 0\n",
    "    best_k = 0\n",
    "    best_feature_set = {}\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        \n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        for c in C_list:\n",
    "            \n",
    "            if model_name == 'SVM':\n",
    "                model = LinearSVC(class_weight='balanced', C=c, penalty=regularization) \n",
    "            elif model_name == 'LR':\n",
    "                model = LogisticRegression(class_weight='balanced', C=c, penalty=regularization)\n",
    "            \n",
    "            for subset in feature_subsets:\n",
    "                X_new = X_kbest_train\n",
    "                for i in subset:\n",
    "                    X_new = hstack([X_new, new_features[i]])\n",
    "                scores = cross_val_score(model, X_new, y_train, cv=k_folds, scoring=scorer)\n",
    "                mean = scores.mean()\n",
    "                if mean > best_score:\n",
    "                    best_score = mean\n",
    "                    best_C = c\n",
    "                    best_k = k_features\n",
    "                    best_feature_set = subset\n",
    "         \n",
    "    return (best_score, best_k, best_C, best_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: BASELINE\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9654    0.8722    0.9164       352\n",
      "          1     0.4886    0.7963    0.6056        54\n",
      "\n",
      "avg / total     0.9020    0.8621    0.8751       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: BASELINE\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7027198542290081\n",
      "Best C: 100\n",
      "Best k: 1500\n",
      "Best new feature subset: (0, 1, 2, 3, 4, 5, 6, 8, 9)\n"
     ]
    }
   ],
   "source": [
    "#Cross validation of SVM\n",
    "#C_range = 10. ** np.arange(-3, 8)\n",
    "#k_range = np.arange(10, 3000, 300)\n",
    "C_range = [0.01, 0.1, 1, 10, 100]\n",
    "k_range = [100, 1500, 3000, 'all']\n",
    "new_features = [X_times_train, X_sem_feat_train_scaled,\n",
    "                X_sentiment_train_scaled, X_subjectivity_train_scaled, X_fp_pronouns_train_scaled,\n",
    "                X_tp_pronouns_train_scaled, X_absolutisms_train_scaled, X_pos_tags_train_scaled,\n",
    "                X_lexicon_sizes_train_scaled, X_post_freq_train_scaled]\n",
    "\n",
    "#best_feature_set vraća kao tuple indexa u listi new_features (gore)\n",
    "best_score, best_k, best_C, best_feature_set = cross_validate_model('SVM', X_train, y_train, C_range,\n",
    "                                                                   k_range, new_features, 4, 9)\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))\n",
    "print(\"Best new feature subset: \" + str(best_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_test = [X_times_test, X_sem_feat_test_scaled,\n",
    "                    X_sentiment_test_scaled, X_subjectivity_test_scaled, X_fp_pronouns_test_scaled,\n",
    "                    X_tp_pronouns_test_scaled, X_absolutisms_test_scaled, X_pos_tags_test_scaled,\n",
    "                    X_lexicon_sizes_test_scaled, X_post_freq_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: SVM classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9300    0.9062    0.9180       352\n",
      "          1     0.4762    0.5556    0.5128        54\n",
      "\n",
      "avg / total     0.8697    0.8596    0.8641       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: SVM classifier\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced', C=100)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = X_kbest_train\n",
    "for i in best_feature_set:\n",
    "    X_final_train = hstack([X_final_train, new_features[i]])\n",
    "    \n",
    "X_final_test = X_kbest_test\n",
    "for i in best_feature_set:\n",
    "    X_final_test = hstack([X_final_test, new_features_test[i]])\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: LogisticRegression classifier\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: Voting classifier (SVM + LR)\")\n",
    "\n",
    "model1 = LogisticRegression(class_weight='balanced')\n",
    "model2 = SVC(class_weight='balanced', kernel='linear', probability=True)\n",
    "model = VotingClassifier(estimators=[('svm', model1), ('lr', model2)], voting='soft', weights=[2, 1])\n",
    "model.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the complete model on whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vstack((X_train_2, X_test_2))\n",
    "y = y_train + y_test\n",
    "\n",
    "model_complete = LinearSVC(class_weight='balanced')\n",
    "model_complete.fit(X, y)\n",
    "\n",
    "print(\"Complete model fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(show_most_informative_features(vect, model_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some baseline classifier testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 2), min_df=20)\n",
    "X_train_3 = vect2.fit_transform(X_train_prep, y_train)\n",
    "X_test_3 = vect2.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: BernoulliNB classifier\")\n",
    "\n",
    "model2 = BernoulliNB()\n",
    "model2.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model2.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: MultinomialNB classifier\")\n",
    "\n",
    "model3 = MultinomialNB()\n",
    "model3.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model3.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: DecisionTree classifier\")\n",
    "\n",
    "model4 = DecisionTreeClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: RandomForest classifier\")\n",
    "\n",
    "model4 = RandomForestClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Empath testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "relevant_lexical_categories1 = ['help', 'medical_emergency', 'hate', 'health', 'suffering', \n",
    "                               'kill', 'fear', 'death', 'violence', 'love',\n",
    "                               'anonymity', 'injury', 'appearance', 'sadness',\n",
    "                               'emotional', 'ugliness', 'shame', 'torment',\n",
    "                               'pain', 'negative_emotion', 'positive_emotion', 'friends',\n",
    "                               'alcohol', 'nervousness', 'optimism', 'body', 'contentment'\n",
    "                               'cold', 'school', 'communication', 'work', 'sleep', 'play'\n",
    "                               'trust', 'social_media', 'sexual'\n",
    "                              ]\n",
    "\n",
    "relevant_lexical_categories = ['negative_emotion', 'speaking', 'positive_emotion', 'communication',\n",
    "                               'friends', 'children', 'optimism', 'violence', 'pain', 'family',\n",
    "                               'trust', 'love', 'party', 'business', 'home', 'shame', 'listen',\n",
    "                               'giving', 'body', 'suffering', 'work', 'nervousness', 'strength',\n",
    "                               'hearing', 'health', 'traveling', 'wedding', 'childish', 'hate',\n",
    "                               'social_media', 'sadness', 'school'\n",
    "                              ]\n",
    "\n",
    "x_senti1 = []\n",
    "y_senti1 = []\n",
    "read_entries(X=x_senti1, y=y_senti1, path_list=test_pos_entry_path_list, default_label=1)\n",
    "\n",
    "x_senti2 = []\n",
    "y_senti2 = []\n",
    "read_entries(X=x_senti2, y=y_senti2, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dict1 = {}\n",
    "len1 = len(x_senti1)\n",
    "for i in x_senti1:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict1 = { k: d.get(k, 0)/len1 + avg_dict1.get(k, 0) for k in set(d) | set(avg_dict1) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict1.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dict2 = {}\n",
    "len2 = len(x_senti2)\n",
    "for i in x_senti2:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict2 = { k: d.get(k, 0)/len2 + avg_dict2.get(k, 0) for k in set(d) | set(avg_dict2) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict2.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(list(lexicon.analyze(x_senti2[2], normalize=True).values()))\n",
    "d = lexicon.analyze(x_senti2[2], normalize=True)\n",
    "for w in sorted(d.keys(), reverse=False):\n",
    "    print(w, d[w])\n",
    "\n",
    "result = [d[key] for key in sorted(d.keys(), reverse=False)]\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen = 'It is not exactly a big deal, but a huge sigh of relief to get confirmation that the movie is on the right track. I love life.'\n",
    "print(sent_tokenize(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_i = []\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i] != y_pred[i]):\n",
    "        user_i.append(i)\n",
    "\n",
    "entry_lists = []\n",
    "path_list = test_pos_entry_path_list + test_neg_entry_path_list\n",
    "for path in path_list:\n",
    "    entry_lists.append(os.scandir(path))\n",
    "    \n",
    "users = []\n",
    "\n",
    "for list_of_entries in entry_lists:\n",
    "    for entry in list_of_entries:\n",
    "        root = etree.parse(entry.path).getroot()\n",
    "        user_id = root[0].text\n",
    "        users.append(user_id)\n",
    "\n",
    "for i in user_i:\n",
    "    print(users[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lexicon.analyze(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = ['I love beinggg retarded']\n",
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "example2 = preprocessor.transform(example, method=preprocess_method)\n",
    "\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
