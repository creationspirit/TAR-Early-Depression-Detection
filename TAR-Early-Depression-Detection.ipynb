{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 136,
=======
   "execution_count": 1347,
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIJENJANO\n",
    "import string\n",
    "import os\n",
    "import regex as re\n",
    "import numpy as np\n",
<<<<<<< HEAD
    "from itertools import combinations, chain\n",
=======
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from lxml import etree\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from scipy.sparse import vstack, hstack, coo_matrix\n",
    "\n",
    "from empath import Empath\n",
    "from textblob import TextBlob"
<<<<<<< HEAD
=======
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle up all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOVO\n",
    "X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\n",
    "X_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\n",
    "X_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\n",
    "X_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\n",
    "X_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\n",
    "\n",
    "X_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\n",
    "X_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\n",
    "X_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\n",
    "X_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\n",
    "X_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\n",
    "\n",
    "X_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\n",
    "X_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\n",
    "\n",
    "X_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\n",
    "X_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle up all data:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
=======
   "execution_count": 747,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\n",
    "X_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\n",
    "X_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\n",
    "X_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\n",
    "X_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\n",
    "\n",
    "X_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\n",
    "X_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\n",
    "X_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\n",
    "X_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\n",
    "X_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\n",
    "\n",
    "X_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\n",
    "X_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\n",
    "\n",
    "X_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\n",
    "X_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Corpus preprocessing:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 137,
   "metadata": {},
=======
   "execution_count": 748,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "sw_diff = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.stopwords.difference_update(sw_diff)\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X, method='lem'):\n",
    "        return [\n",
    "            list(self.tokenize(doc, method)) for doc in X\n",
    "        ]   \n",
    "\n",
    "    def tokenize(self, document, method='lem'):\n",
    "        if(method == 'lem'):\n",
    "            # Break the document into sentences\n",
    "            for sent in sent_tokenize(document):\n",
    "                # Break the sentence into part of speech tagged tokens\n",
    "                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                    # Apply preprocessing to the token\n",
    "                    token = self.process_token(token)\n",
    "                    if not self.is_valid_token(token):\n",
    "                        continue\n",
    "                        \n",
    "                    # Lemmatize the token and yield\n",
    "                    lemma = self.lemmatize(token, tag)\n",
    "                    yield lemma\n",
    "                    \n",
    "        elif(method == 'stem'):\n",
    "            # Break the document into tokens\n",
    "            for token in wordpunct_tokenize(document):\n",
    "                # Apply preprocessing to the token\n",
    "                token = self.process_token(token)\n",
    "                if not self.is_valid_token(token):\n",
    "                    continue\n",
    "                \n",
    "                stem = self.stem(token)\n",
    "                yield stem\n",
    "        else:\n",
    "            raise ValueError('Unknown method type.')\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def stem(self, token):\n",
    "        return self.stemmer.stem(token)\n",
    "    \n",
    "    def process_token(self, token):\n",
    "        token = token.lower() if self.lower else token\n",
    "        token = token.strip() if self.strip else tcharoken\n",
    "        token = token.strip('_') if self.strip else token\n",
    "        token = token.strip('*') if self.strip else token\n",
    "        return token\n",
    "    \n",
    "    def is_valid_token(self, token):\n",
    "        # If stopword, token is invalid\n",
    "        if token in self.stopwords:\n",
    "            return False\n",
    "\n",
    "        # If punctuation, token is invalid\n",
    "        if all(char in self.punct for char in token):\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code loads data corpus from multiple files into lists X (texts) and y(labels) with one entry per user:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 138,
   "metadata": {},
=======
   "execution_count": 1141,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "def read_entries(X, y, path_list, label_dict=None, default_label=0):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "    \n",
    "    IMAGE_STR = 'data:image'\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "        \n",
    "            user_text = ''\n",
    "            for post in root.findall('.//TITLE') + root.findall('.//TEXT'):\n",
    "                post = post.text.strip().strip()\n",
    "                if post != '':\n",
    "                    if IMAGE_STR in post:\n",
    "                        continue\n",
    "                    post = re.sub(r\"http\\S+\", \" \", post)\n",
    "                    post = re.sub(r\"\\d+\", \" \", post)\n",
    "                    post = re.sub(u\"\\xa0\", \" \", post)\n",
    "                    post = re.sub(u\"\\\\p{P}+\", \" \", post)\n",
    "                    user_text += ' ' + post.lower()\n",
    "            \n",
    "            X.append(user_text)\n",
    "            label = int(label_dict[user_id]) if label_dict else default_label\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility methods for extracting features:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 139,
   "metadata": {},
=======
   "execution_count": 1142,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "def get_avg_sentence_length(sentences):\n",
    "    sum = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(' ', '')\n",
    "        sum += len(sentence)\n",
    "    return sum / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_and_subjectivity(sentences):\n",
    "    sum_sentiment = 0\n",
    "    sum_subjectivity = 0\n",
    "    if len(sentences) > 0:\n",
    "        for sentence in sentences:\n",
    "            tb = TextBlob(sentence)\n",
    "            sum_sentiment += tb.sentiment.polarity\n",
    "            sum_subjectivity += tb.sentiment.subjectivity\n",
    "        sum_sentiment = sum_sentiment / float(len(sentences))\n",
    "        sum_subjectivity = sum_subjectivity / float(len(sentences))\n",
    "        return (sum_sentiment, sum_subjectivity)\n",
    "    else:\n",
    "        return (0.0, 0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 1143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentiment_and_subjectivity(sentences):\n",
    "    sum_sentiment = 0\n",
    "    sum_subjectivity = 0\n",
    "    if sentences:\n",
    "        for sentence in sentences:\n",
    "            tb = TextBlob(sentence)\n",
    "            sum_sentiment += tb.sentiment.polarity\n",
    "            sum_subjectivity += tb.sentiment.subjectivity\n",
    "        sum_sentiment = sum_sentiment / float(len(sentences))\n",
    "        sum_subjectivity = sum_subjectivity / float(len(sentences))\n",
    "        return (sum_sentiment, sum_subjectivity)\n",
    "    else:\n",
    "        return (0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MIJENJANO\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "def read_features(X_times, X_sentence_lengths, X_post_cnt, X_sentiment,\n",
    "                  X_subjectivity, X_post_lengths, X_post_freq, path_list):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "        \n",
    "    IMAGE_STR = 'data:image'\n",
    "    datetime_pattern = '%Y-%m-%d %H:%M:%S'\n",
    "    date_end = None\n",
    "    date_start = None\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "            \n",
    "            sentences = []\n",
    "            post_lengths = []\n",
    "            post_cnt = 0\n",
    "            for post in root.findall('.//TEXT'):\n",
    "                post_cnt += 1\n",
    "                post = post.text.strip()\n",
    "                if post != '':\n",
    "                    sentences.extend(sent_tokenize(post))\n",
    "                    post_lengths.append(len(post))\n",
    "                else:\n",
    "                    post_lengths.append(0)\n",
    "            \n",
<<<<<<< HEAD
    "            avg_sentiment, avg_subjectivity = get_sentiment_and_subjectivity(sentences)\n",
=======
    "            #avg_sentiment, avg_subjectivity = get_sentiment_and_subjectivity(sentences)\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "            avg_sentence_length = get_avg_sentence_length(sentences)\n",
    "            avg_post_length = np.mean(post_lengths)\n",
    "            \n",
    "            sum_hours = 0\n",
    "            for date in root.findall('.//DATE'):\n",
    "                date = date.text.strip()\n",
    "                if date != '':                    \n",
    "                    if not date_end:\n",
    "                        date_end = datetime.strptime(date, datetime_pattern)\n",
    "                    date_start = datetime.strptime(date, datetime_pattern)\n",
    "                    \n",
    "                    m = re.match(r'\\d{4}-\\d{2}-\\d{2} (\\d{2}).*', date)\n",
    "                    hour = int(m.group(1))\n",
    "                    sum_hours += hour\n",
    "            \n",
<<<<<<< HEAD
    "            post_span_minutes = (date_end - date_start).total_seconds()/60\n",
=======
    "            post_span_minutes = (datetime_end - datetime_start).total_seconds()/60\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "            post_freq = post_span_minutes / post_cnt\n",
    "            \n",
    "            time = [0] * 8\n",
    "            avg_hour = sum_hours / post_cnt\n",
    "            index = int(avg_hour // 3)\n",
    "            time[index] = 1\n",
    "            \n",
    "            X_post_cnt.append([post_cnt])\n",
    "            X_sentence_lengths.append([avg_sentence_length])\n",
    "            X_times.append(time)\n",
    "            X_sentiment.append([avg_sentiment])\n",
    "            X_subjectivity.append([avg_subjectivity])\n",
    "            X_post_lengths.append([avg_post_length])\n",
    "            X_post_freq.append([post_freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading input files:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 142,
   "metadata": {},
=======
   "execution_count": 1145,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "TRAIN_PATH = os.path.join(cwd, \"reddit-training-ready-to-share\")\n",
    "TEST_PATH = os.path.join(cwd, \"reddit-test-data-ready-to-share\")\n",
    "\n",
    "TRAIN_POSITIVE_PATH = os.path.join(TRAIN_PATH, \"positive_examples_anonymous\")\n",
    "TRAIN_NEGATIVE_PATH = os.path.join(TRAIN_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TEST_POSITIVE_PATH = os.path.join(TEST_PATH, \"positive_examples_anonymous\")\n",
    "TEST_NEGATIVE_PATH = os.path.join(TEST_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TRAIN_LABELS_PATH = os.path.join(cwd, 'risk_golden_truth.txt')\n",
    "\n",
    "IMAGE_STR = 'data:image'\n",
    "\n",
    "train_labels_file = open(TRAIN_LABELS_PATH, 'r')\n",
    "train_label_dict = {}\n",
    "for line in train_labels_file:\n",
    "    xml_file, label = line.split(' ')\n",
    "    train_label_dict[xml_file] = label\n",
    "train_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 143,
   "metadata": {},
=======
   "execution_count": 1146,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "X_train_raw = []\n",
    "y_train = []\n",
    "X_test_raw = []\n",
    "y_test = []\n",
    "\n",
    "train_entry_path_list = [TRAIN_POSITIVE_PATH, TRAIN_NEGATIVE_PATH]\n",
    "test_pos_entry_path_list = [TEST_POSITIVE_PATH]\n",
    "test_neg_entry_path_list = [TEST_NEGATIVE_PATH]\n",
    "\n",
    "read_entries(X=X_train_raw, y=y_train, path_list=train_entry_path_list, label_dict=train_label_dict)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_pos_entry_path_list, default_label=1)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_and_absolutizm_features(X_raw):\n",
    "    \n",
    "    fp_pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    \n",
    "    tp_pronouns = {'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "    \n",
    "    absolutisms = {'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "                   'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', \n",
    "                   'totally', 'whole', 'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "\n",
    "    fp_freq = []\n",
    "    tp_freq = []\n",
    "    absolutisms_freq = []\n",
    "    \n",
    "    for entry in X_raw:\n",
    "        sum_fp = 0\n",
    "        sum_tp = 0\n",
    "        sum_abs = 0\n",
    "        tokens = word_tokenize(entry)\n",
    "        for word in tokens:\n",
    "            if word in fp_pronouns:\n",
    "                sum_fp += 1\n",
    "            elif word in tp_pronouns:\n",
    "                sum_tp += 1\n",
    "            elif word in absolutisms:\n",
    "                sum_abs += 1\n",
    "        sum_fp = sum_fp / float(len(tokens))\n",
    "        sum_tp = sum_tp / float(len(tokens))\n",
    "        sum_abs = sum_abs / float(len(tokens))\n",
    "        fp_freq.append([sum_fp])\n",
    "        tp_freq.append([sum_tp])\n",
    "        absolutisms_freq.append([sum_abs])\n",
    "    \n",
    "    return (fp_freq, tp_freq, absolutisms_freq)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 147,
=======
   "execution_count": 1362,
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIJENJANO\n",
    "X_times_train = []\n",
    "X_times_test = []\n",
    "\n",
    "X_sentence_len_train = []\n",
    "X_sentence_len_test = []\n",
    "\n",
    "X_post_cnt_train = []\n",
    "X_post_cnt_test = []\n",
    "\n",
    "X_sentiment_train = []\n",
    "X_sentiment_test = []\n",
    "\n",
    "X_subjectivity_train = []\n",
    "X_subjectivity_test = []\n",
    "\n",
    "X_post_lengths_train = []\n",
    "X_post_lengths_test = []\n",
    "\n",
    "X_post_freq_train = []\n",
    "X_post_freq_test = []\n",
    "\n",
    "read_features(X_times=X_times_train, X_sentence_lengths=X_sentence_len_train, X_post_cnt=X_post_cnt_train,\n",
    "              X_sentiment=X_sentiment_train, X_subjectivity=X_subjectivity_train, X_post_lengths=X_post_lengths_train,\n",
    "              X_post_freq=X_post_freq_train, path_list=train_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_pos_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_neg_entry_path_list)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fp_pronouns_train, X_tp_pronouns_train, X_absolutisms_train = get_pronoun_and_absolutizm_features(X_train_raw)\n",
    "X_fp_pronouns_test, X_tp_pronouns_test, X_absolutisms_test = get_pronoun_and_absolutizm_features(X_test_raw)"
=======
   "execution_count": 1370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOVO\n",
    "pickle.dump(X_times_train, open(\"./dumps/X_times_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_train, open(\"./dumps/X_sentence_len_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_train, open(\"./dumps/X_post_cnt_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_train, open(\"./dumps/X_sentiment_train.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_train, open(\"./dumps/X_subjectivity_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_train, open(\"./dumps/X_post_lengths_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_train, open(\"./dumps/X_post_freq_train.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_times_test, open(\"./dumps/X_times_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_test, open(\"./dumps/X_sentence_len_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_test, open(\"./dumps/X_post_cnt_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_test, open(\"./dumps/X_sentiment_test.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_test, open(\"./dumps/X_subjectivity_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_test, open(\"./dumps/X_post_lengths_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_test, open(\"./dumps/X_post_freq_test.p\", \"wb\" ))"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 25,
   "metadata": {},
=======
   "execution_count": 1232,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "def get_semantic_features(X):\n",
    "    lexicon = Empath()\n",
    "    \n",
    "    relevant_lexical_categories = ['negative_emotion', 'positive_emotion', 'communication',\n",
    "                                    'violence', 'business', 'nervousness', 'body', 'pain',\n",
    "                                    'internet', 'work', 'shame', 'poor'\n",
    "                              ]\n",
    "    \n",
    "    relevant_lexical_categories2 = ['negative_emotion', 'positive_emotion',\n",
    "                                   'nervousness', 'love', 'shame', 'pain'\n",
    "                              ]\n",
    "    \n",
    "    feature_mat = []\n",
    "    for text in X:\n",
    "        d = lexicon.analyze(text, categories=relevant_lexical_categories2, normalize=True)\n",
    "        feature_mat.append([d[key] for key in sorted(d.keys(), reverse=False)])\n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
   "metadata": {},
=======
   "execution_count": 1233,
   "metadata": {
    "collapsed": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [],
   "source": [
    "X_sem_feat_train = get_semantic_features(X_train_raw)\n",
    "X_sem_feat_test = get_semantic_features(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_times_train, open(\"./dumps/X_times_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_train, open(\"./dumps/X_sentence_len_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_train, open(\"./dumps/X_post_cnt_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_train, open(\"./dumps/X_sentiment_train.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_train, open(\"./dumps/X_subjectivity_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_train, open(\"./dumps/X_post_lengths_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_train, open(\"./dumps/X_post_freq_train.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_train, open(\"./dumps/X_fp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_train, open(\"./dumps/X_tp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_train, open(\"./dumps/X_absolutisms_train.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_times_test, open(\"./dumps/X_times_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_test, open(\"./dumps/X_sentence_len_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_test, open(\"./dumps/X_post_cnt_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_test, open(\"./dumps/X_sentiment_test.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_test, open(\"./dumps/X_subjectivity_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_test, open(\"./dumps/X_post_lengths_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_test, open(\"./dumps/X_post_freq_test.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_test, open(\"./dumps/X_fp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_test, open(\"./dumps/X_tp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_test, open(\"./dumps/X_absolutisms_test.p\", \"wb\" ))\n",
    "\n",
=======
   "execution_count": 1371,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOVO\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "pickle.dump(X_sem_feat_train, open(\"./dumps/X_sem_feat_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sem_feat_test, open(\"./dumps/X_sem_feat_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use X list as input to NLTKPreprocessor class which outputs list of preprocessed, tokenized texts:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 27,
=======
   "execution_count": 717,
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "X_train_prep = preprocessor.transform(X_train_raw, method=preprocess_method)\n",
    "X_test_prep = preprocessor.transform(X_test_raw, method=preprocess_method)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
=======
   "execution_count": 1191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOVO\n",
    "def get_pos_tags(X):\n",
    "    pos_tag_mat = []\n",
    "    for tokens in X:\n",
    "        tag_dict = { 'CC': 0, 'DT': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, 'JJS': 0,\n",
    "                    'NN': 0, 'NNP':0, 'NNS': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0,\n",
    "                    'RBR': 0, 'RBS': 0, 'RP': 0, 'VB': 0, 'VBD': 0, 'VBG': 0,\n",
    "                    'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        \n",
    "        text_len = len(tokens)\n",
    "        tags = pos_tag(tokens)\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if tag in tag_dict.keys():\n",
    "                tag_dict[tag] += 1/text_len\n",
    "        \n",
    "        tag_freq = [tag_dict[key] for key in sorted(tag_dict.keys(), reverse=False)]\n",
    "        pos_tag_mat.append(tag_freq)\n",
    "    \n",
    "    return pos_tag_mat"
   ]
  },
  {
   "cell_type": "code",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "def get_pos_tags(X):\n",
    "    pos_tag_mat = []\n",
    "    for tokens in X:\n",
    "        tag_dict = { 'CC': 0, 'DT': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, 'JJS': 0,\n",
    "                    'NN': 0, 'NNP':0, 'NNS': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0,\n",
    "                    'RBR': 0, 'RBS': 0, 'RP': 0, 'VB': 0, 'VBD': 0, 'VBG': 0,\n",
    "                    'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        \n",
    "        text_len = len(tokens)\n",
    "        tags = pos_tag(tokens)\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if tag in tag_dict.keys():\n",
    "                tag_dict[tag] += 1/text_len\n",
    "        \n",
    "        tag_freq = [tag_dict[key] for key in sorted(tag_dict.keys(), reverse=False)]\n",
    "        pos_tag_mat.append(tag_freq)\n",
    "    \n",
    "    return pos_tag_mat"
=======
    "# NOVO\n",
    "X_pos_tags_train = get_pos_tags(X_train_prep)\n",
    "X_pos_tags_test = get_pos_tags(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOVO\n",
    "pickle.dump(X_pos_tags_train, open(\"./dumps/X_pos_tags_train.p\", \"wb\" ))\n",
    "pickle.dump(X_pos_tags_test, open(\"./dumps/X_pos_tags_test.p\", \"wb\" ))"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_tags_train = get_pos_tags(X_train_prep)\n",
    "X_pos_tags_test = get_pos_tags(X_test_prep)"
=======
   "execution_count": 1194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOVO\n",
    "def get_lexicon_sizes(X):\n",
    "    unique_cnt_mat = []\n",
    "    for tokens in X:\n",
    "        unique_cnt_mat.append([len(set(tokens))])\n",
    "    \n",
    "    return unique_cnt_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOVO\n",
    "X_lexicon_sizes_train = get_lexicon_sizes(X_train_prep)\n",
    "X_lexicon_sizes_test = get_lexicon_sizes(X_test_prep)"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_pos_tags_train, open(\"./dumps/X_pos_tags_train.p\", \"wb\" ))\n",
    "pickle.dump(X_pos_tags_test, open(\"./dumps/X_pos_tags_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_sizes(X):\n",
    "    unique_cnt_mat = []\n",
    "    for tokens in X:\n",
    "        unique_cnt_mat.append([len(set(tokens))])\n",
    "    \n",
    "    return unique_cnt_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lexicon_sizes_train = get_lexicon_sizes(X_train_prep)\n",
    "X_lexicon_sizes_test = get_lexicon_sizes(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
   "execution_count": 1373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOVO\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "pickle.dump(X_lexicon_sizes_train, open(\"./dumps/X_lexicon_sizes_train.p\", \"wb\" ))\n",
    "pickle.dump(X_lexicon_sizes_test, open(\"./dumps/X_lexicon_sizes_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tf-idf vectorizer for vector representation of the documents:"
<<<<<<< HEAD
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 1), min_df=30)\n",
    "X_train = vect.fit_transform(X_train_prep, y_train)\n",
    "X_test = vect.transform(X_test_prep)"
=======
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 88,
=======
   "execution_count": 1084,
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#print(vect.get_feature_names())"
=======
    "vect = TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 1), min_df=30)\n",
    "X_train = vect.fit_transform(X_train_prep, y_train)\n",
    "X_test = vect.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_selector = SelectKBest(chi2, k='all')\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and adding features:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 73,
=======
   "execution_count": 1363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZANEMARI OVO\n",
    "X_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\n",
    "X_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\n",
    "\n",
    "X_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\n",
    "X_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1364,
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIJENJANO\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "\n",
    "X_sentence_len_train_scaled = max_abs_scaler.fit_transform(X_sentence_len_train)\n",
    "X_sentence_len_test_scaled = max_abs_scaler.transform(X_sentence_len_test)\n",
    "\n",
    "X_sem_feat_train_scaled = max_abs_scaler.fit_transform(X_sem_feat_train)\n",
    "X_sem_feat_test_scaled = max_abs_scaler.transform(X_sem_feat_test)\n",
    "\n",
    "X_post_cnt_train_scaled = max_abs_scaler.fit_transform(X_post_cnt_train)\n",
    "X_post_cnt_test_scaled = max_abs_scaler.transform(X_post_cnt_test)\n",
    "\n",
    "X_sentiment_train_scaled = max_abs_scaler.fit_transform(X_sentiment_train)\n",
    "X_sentiment_test_scaled = max_abs_scaler.transform(X_sentiment_test)\n",
    "\n",
    "X_subjectivity_train_scaled = max_abs_scaler.fit_transform(X_subjectivity_train)\n",
    "X_subjectivity_test_scaled = max_abs_scaler.transform(X_subjectivity_test)\n",
    "\n",
<<<<<<< HEAD
    "X_fp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_fp_pronouns_train)\n",
    "X_fp_pronouns_test_scaled = max_abs_scaler.transform(X_fp_pronouns_test)\n",
    "\n",
    "X_tp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_tp_pronouns_train)\n",
    "X_tp_pronouns_test_scaled = max_abs_scaler.transform(X_tp_pronouns_test)\n",
    "\n",
    "X_absolutisms_train_scaled = max_abs_scaler.fit_transform(X_absolutisms_train)\n",
    "X_absolutisms_test_scaled = max_abs_scaler.transform(X_absolutisms_test)\n",
    "\n",
=======
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "X_pos_tags_train_scaled = max_abs_scaler.fit_transform(X_pos_tags_train)\n",
    "X_pos_tags_test_scaled = max_abs_scaler.fit_transform(X_pos_tags_test)\n",
    "\n",
    "X_lexicon_sizes_train_scaled = max_abs_scaler.fit_transform(X_lexicon_sizes_train)\n",
    "X_lexicon_sizes_test_scaled = max_abs_scaler.fit_transform(X_lexicon_sizes_test)\n",
    "\n",
    "X_post_lengths_train_scaled = max_abs_scaler.fit_transform(X_post_lengths_train)\n",
    "X_post_lengths_test_scaled = max_abs_scaler.fit_transform(X_post_lengths_test)\n",
    "\n",
    "X_post_freq_train_scaled = max_abs_scaler.fit_transform(X_post_freq_train)\n",
    "X_post_freq_test_scaled = max_abs_scaler.fit_transform(X_post_freq_test)"
<<<<<<< HEAD
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and evaluating models:"
=======
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find allsubsets of a set (needed to find all subsets of set of new feature for CV)\n",
    "allsubsets = lambda n: list(chain(*[combinations(range(n), ni) for ni in range(n+1)]))\n",
    "\n",
    "# r - return only subsets of r size (reduction of search space) + empty set; if None, return all subsets\n",
    "def get_subsets(n, r=None):\n",
    "    if r==None:\n",
    "        return allsubsets(n)\n",
    "    else:\n",
    "        combs = list(combinations(range(n), r))\n",
    "        combs.append(())\n",
    "        return combs"
=======
   "execution_count": 1365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIJENJANO\n",
    "X_train_2 = hstack([X_kbest_train, X_times_train, X_sentence_len_train_scaled,\n",
    "                    X_sem_feat_train_scaled, X_sentiment_train_scaled,\n",
    "                    X_subjectivity_train_scaled, X_pos_tags_train_scaled, \n",
    "                    X_lexicon_sizes_train_scaled, X_post_lengths_train_scaled,\n",
    "                    X_post_freq_train_scaled\n",
    "                   ])\n",
    "\n",
    "X_test_2 = hstack([X_kbest_test, X_times_test, X_sentence_len_test_scaled,\n",
    "                   X_sem_feat_test_scaled, X_sentiment_test_scaled,\n",
    "                   X_subjectivity_test_scaled, X_pos_tags_test_scaled, \n",
    "                   X_lexicon_sizes_test_scaled, X_post_lengths_test_scaled,\n",
    "                   X_post_freq_test_scaled\n",
    "                  ])"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for k-fold cross-validation of a model\n",
    "#Performs grid search on C parameter, subsets of new features and k-best token features by chi2 stat\n",
    "#For model_name parameter use 'SVM' - SVM or 'LR' - LogisticRegression\n",
    "\n",
    "#EXAMPLE: see example below baseline for CV of SVM model\n",
    "def cross_validate_model(model_name, X_train, y_train, C_list, k_list, new_features, k_folds):\n",
    "    feature_subsets = get_subsets(len(new_features), 6)\n",
    "    best_C = 0\n",
    "    best_k = 0\n",
    "    best_feature_set = {}\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        \n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        for c in C_list:\n",
    "            \n",
    "            if model_name == 'SVM':\n",
    "                model = LinearSVC(class_weight='balanced', C=c) \n",
    "            elif model_name == 'LR':\n",
    "                model = LogisticRegression(class_weight='balanced', C=c)\n",
    "            \n",
    "            for subset in feature_subsets:\n",
    "                X_new = X_kbest_train\n",
    "                for i in subset:\n",
    "                    X_new = hstack([X_new, new_features[i]])\n",
    "                scores = cross_val_score(model, X_new, y_train, cv=k_folds, scoring=scorer)\n",
    "                mean = scores.mean()\n",
    "                if mean > best_score:\n",
    "                    best_score = mean\n",
    "                    best_C = c\n",
    "                    best_k = k_features\n",
    "                    best_feature_set = subset\n",
    "         \n",
    "    return (best_score, best_k, best_C, best_feature_set)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 129,
   "metadata": {},
=======
   "execution_count": 1368,
   "metadata": {
    "scrolled": true
   },
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Building for evaluation: BASELINE\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9687    0.8778    0.9210       352\n",
      "          1     0.5057    0.8148    0.6241        54\n",
      "\n",
      "avg / total     0.9071    0.8695    0.8815       406\n",
      "\n"
=======
      "Cross validation for positive label scores:\n",
      "\tAverage precision score:\n",
      "\t\t 0.646544566545\n",
      "\tAverage recall score:\n",
      "\t\t 0.675\n",
      "\tAverage  F1 score:\n",
      "\t\t 0.649131161236\n",
      "\n",
      "Cross validation for all label scores:\n",
      "\tAverage precision score:\n",
      "\t\t 0.789805108777\n",
      "\tAverage recall score:\n",
      "\t\t 0.796463414634\n",
      "\tAverage  F1 score:\n",
      "\t\t 0.786856949586\n"
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(\"Building for evaluation: BASELINE\")\n",
=======
    "model = LinearSVC(class_weight='balanced', C=5)\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "\n",
    "model = LinearSVC(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-50f1933e711c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m best_score, best_k, best_C, best_feature_set = cross_validate_model('SVM', X_train, y_train, C_range,\n\u001b[0;32m---> 11\u001b[0;31m                                                                    k_range, new_features, 5)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best score: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-123-f1459181c499>\u001b[0m in \u001b[0;36mcross_validate_model\u001b[0;34m(model_name, X_train, y_train, C_list, k_list, new_features, k_folds)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    891\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Cross validation of SVM\n",
    "#C_range = 10. ** np.arange(-3, 8)\n",
    "#k_range = np.arange(10, 3000, 300)\n",
    "C_range = [0.01, 0.1, 1, 10, 100]\n",
    "k_range = [100, 1500, 3000]\n",
    "new_features = [X_times_train, X_sentence_len_train_scaled, X_sem_feat_train_scaled, X_post_cnt_train_scaled,\n",
    "                X_sentiment_train_scaled, X_subjectivity_train_scaled, X_fp_pronouns_train_scaled,\n",
    "                X_tp_pronouns_train_scaled, X_absolutisms_train_scaled]\n",
    "\n",
    "#best_feature_set vraa kao tuple indexa u listi new_features (gore)\n",
    "best_score, best_k, best_C, best_feature_set = cross_validate_model('SVM', X_train, y_train, C_range,\n",
    "                                                                   k_range, new_features, 5)\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))\n",
    "print(\"Best new feature subset: \" + str(best_feature_set))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 135,
=======
   "execution_count": 1369,
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: SVM classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
<<<<<<< HEAD
      "          0     0.9472    0.8665    0.9050       352\n",
      "          1     0.4405    0.6852    0.5362        54\n",
      "\n",
      "avg / total     0.8798    0.8424    0.8560       406\n",
=======
      "          0     0.9540    0.8835    0.9174       352\n",
      "          1     0.4875    0.7222    0.5821        54\n",
      "\n",
      "avg / total     0.8919    0.8621    0.8728       406\n",
      "\n",
      "Building for evaluation: SVM classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9687    0.8778    0.9210       352\n",
      "          1     0.5057    0.8148    0.6241        54\n",
      "\n",
      "avg / total     0.9071    0.8695    0.8815       406\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
      "\n"
     ]
    }
   ],
   "source": [
    "# MIJENJANO\n",
    "print(\"Building for evaluation: SVM classifier\")\n",
    "\n",
<<<<<<< HEAD
    "model = LinearSVC(class_weight='balanced', C=10)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=100)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_train_2 = hstack([X_kbest_train, X_times_train, X_sem_feat_train_scaled, X_sentiment_train_scaled,\n",
    "                    X_fp_pronouns_train_scaled, X_tp_pronouns_train_scaled, X_absolutisms_train_scaled])\n",
    "\n",
    "X_test_2 = hstack([X_kbest_test, X_times_test, X_sem_feat_test_scaled, X_sentiment_test_scaled, \n",
    "                   X_fp_pronouns_test_scaled, X_tp_pronouns_test_scaled, X_absolutisms_test_scaled])\n",
    "\n",
=======
    "model = LinearSVC(class_weight='balanced', C=1)\n",
>>>>>>> 84b8c7a51ff2d3732538279fb944d88b667c6587
    "model.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "print(\"Building for evaluation: SVM classifier\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation for positive label scores:\n",
      "\tAverage precision score:\n",
      "\t\t 0.361268459611\n",
      "\tAverage recall score:\n",
      "\t\t 0.855555555556\n",
      "\tAverage  F1 score:\n",
      "\t\t 0.504187702933\n",
      "\n",
      "Cross validation for all label scores:\n",
      "\tAverage precision score:\n",
      "\t\t 0.66030034177\n",
      "\tAverage recall score:\n",
      "\t\t 0.766497289973\n",
      "\tAverage  F1 score:\n",
      "\t\t 0.647788931658\n"
     ]
    }
   ],
   "source": [
    "model2 = LogisticRegression(class_weight='balanced', C=0.1)\n",
    "\n",
    "print(\"Cross validation for positive label scores:\")\n",
    "print(\"\\tAverage precision score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model2, X_train_2, y_train, cv=10, scoring=make_scorer(precision_score, average='macro', labels=[1]))))\n",
    "print(\"\\tAverage recall score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model2, X_train_2, y_train, cv=10, scoring=make_scorer(recall_score, average='macro', labels=[1]))))\n",
    "print(\"\\tAverage  F1 score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model2, X_train_2, y_train, cv=10, scoring=make_scorer(f1_score, average='macro', labels=[1]))))\n",
    "\n",
    "print()\n",
    "print(\"Cross validation for all label scores:\")\n",
    "print(\"\\tAverage precision score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model2, X_train_2, y_train, cv=10, scoring=make_scorer(precision_score, average='macro'))))\n",
    "print(\"\\tAverage recall score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model2, X_train_2, y_train, cv=10, scoring=make_scorer(recall_score, average='macro'))))\n",
    "print(\"\\tAverage  F1 score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model2, X_train_2, y_train, cv=10, scoring=make_scorer(f1_score, average='macro'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = LogisticRegression(class_weight='balanced')\n",
    "model__ = SVC(class_weight='balanced', kernel='linear', probability=True)\n",
    "model3 = VotingClassifier(estimators=[('svm', model_), ('lr', model__)], voting='soft', weights=[2, 1])\n",
    "\n",
    "print(\"Cross validation for positive label scores:\")\n",
    "print(\"\\tAverage precision score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model3, X_train_2, y_train, cv=10, scoring=make_scorer(precision_score, average='macro', labels=[1]))))\n",
    "print(\"\\tAverage recall score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model3, X_train_2, y_train, cv=10, scoring=make_scorer(recall_score, average='macro', labels=[1]))))\n",
    "print(\"\\tAverage  F1 score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model3, X_train_2, y_train, cv=10, scoring=make_scorer(f1_score, average='macro', labels=[1]))))\n",
    "\n",
    "print()\n",
    "print(\"Cross validation for all label scores:\")\n",
    "print(\"\\tAverage precision score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model3, X_train_2, y_train, cv=10, scoring=make_scorer(precision_score, average='macro'))))\n",
    "print(\"\\tAverage recall score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model3, X_train_2, y_train, cv=10, scoring=make_scorer(recall_score, average='macro'))))\n",
    "print(\"\\tAverage  F1 score:\")\n",
    "print('\\t\\t', np.mean(cross_val_score(model3, X_train_2, y_train, cv=10, scoring=make_scorer(f1_score, average='macro'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: LogisticRegression classifier\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: Voting classifier (SVM + LR)\")\n",
    "\n",
    "model1 = LogisticRegression(class_weight='balanced')\n",
    "model2 = SVC(class_weight='balanced', kernel='linear', probability=True)\n",
    "model = VotingClassifier(estimators=[('svm', model1), ('lr', model2)], voting='soft', weights=[2, 1])\n",
    "model.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the complete model on whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vstack((X_train_2, X_test_2))\n",
    "y = y_train + y_test\n",
    "\n",
    "model_complete = LinearSVC(class_weight='balanced')\n",
    "model_complete.fit(X, y)\n",
    "\n",
    "print(\"Complete model fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(show_most_informative_features(vect, model_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some baseline classifier testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 2), min_df=20)\n",
    "X_train_3 = vect2.fit_transform(X_train_prep, y_train)\n",
    "X_test_3 = vect2.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: BernoulliNB classifier\")\n",
    "\n",
    "model2 = BernoulliNB()\n",
    "model2.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model2.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: MultinomialNB classifier\")\n",
    "\n",
    "model3 = MultinomialNB()\n",
    "model3.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model3.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: DecisionTree classifier\")\n",
    "\n",
    "model4 = DecisionTreeClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: RandomForest classifier\")\n",
    "\n",
    "model4 = RandomForestClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Empath testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "\n",
    "x_senti1 = []\n",
    "y_senti1 = []\n",
    "read_entries(X=x_senti1, y=y_senti1, path_list=test_pos_entry_path_list, default_label=1)\n",
    "\n",
    "x_senti2 = []\n",
    "y_senti2 = []\n",
    "read_entries(X=x_senti2, y=y_senti2, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dict1 = {}\n",
    "len1 = len(x_senti1)\n",
    "for i in x_senti1:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict1 = { k: d.get(k, 0)/len1 + avg_dict1.get(k, 0) for k in set(d) | set(avg_dict1) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict1.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dict2 = {}\n",
    "len2 = len(x_senti2)\n",
    "for i in x_senti2:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict2 = { k: d.get(k, 0)/len2 + avg_dict2.get(k, 0) for k in set(d) | set(avg_dict2) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict2.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_dict = {}\n",
    "for k, v in avg_dict1.items():\n",
    "    diff_dict[k] = abs(v - avg_dict2[k])\n",
    "\n",
    "for k, v in sorted(diff_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(lexicon.analyze(x_senti2[2], normalize=True).values()))\n",
    "d = lexicon.analyze(x_senti2[2], normalize=True)\n",
    "for w in sorted(d.keys(), reverse=False):\n",
    "    print(w, d[w])\n",
    "\n",
    "result = [d[key] for key in sorted(d.keys(), reverse=False)]\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_2)\n",
    "user_i = []\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i] != y_pred[i]):\n",
    "        user_i.append(i)\n",
    "\n",
    "entry_lists = []\n",
    "path_list = test_pos_entry_path_list + test_neg_entry_path_list\n",
    "for path in path_list:\n",
    "    entry_lists.append(os.scandir(path))\n",
    "    \n",
    "users = []\n",
    "\n",
    "for list_of_entries in entry_lists:\n",
    "    for entry in list_of_entries:\n",
    "        root = etree.parse(entry.path).getroot()\n",
    "        user_id = root[0].text\n",
    "        users.append(user_id)\n",
    "\n",
    "for i in user_i:\n",
    "    print(users[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0.5, 0, 0.25, 0, 0, 0, 0, 0, 0, 0, 0, 0.25, 0]\n"
     ]
    }
   ],
   "source": [
    "example = ['I soaked my pants']\n",
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "example2 = preprocessor.transform(example, method=preprocess_method)\n",
    "\n",
    "for i in example2:\n",
    "    text_len = len(i)\n",
    "    tags = pos_tag(i)\n",
    "    tag_dict = { 'CC': 0, 'DT': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, 'JJS': 0,\n",
    "                'NN': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0, 'RBR': 0, 'RBS': 0, 'RP': 0,\n",
    "                'VB': 0, 'VBD': 0, 'VBG': 0, 'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "    for word, tag in tags:\n",
    "        tag_dict[tag] += 1/text_len\n",
    "    tag_freq = [tag_dict[key] for key in sorted(tag_dict.keys(), reverse=False)]\n",
    "    print(tag_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    print(i, (i+4)%24 // 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
