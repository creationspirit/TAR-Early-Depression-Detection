{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from itertools import combinations, chain\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from lxml import etree\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag, FreqDist\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, RFECV, SelectFromModel\n",
    "from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier\n",
    "\n",
    "from scipy.sparse import vstack, hstack, coo_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from empath import Empath\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle up all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\\nX_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\\nX_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\\nX_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\\nX_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\\n\\nX_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\\nX_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\\nX_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\\nX_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\\nX_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\\n\\nX_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\\nX_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\\n\\nX_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\\nX_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''X_times_train = pickle.load(open(\"./dumps/X_times_train.p\", \"rb\" ))\n",
    "X_sentence_len_train = pickle.load(open(\"./dumps/X_sentence_len_train.p\", \"rb\" ))\n",
    "X_post_cnt_train = pickle.load(open(\"./dumps/X_post_cnt_train.p\", \"rb\" ))\n",
    "X_sentiment_train = pickle.load(open(\"./dumps/X_sentiment_train.p\", \"rb\" ))\n",
    "X_subjectivity_train = pickle.load(open(\"./dumps/X_subjectivity_train.p\", \"rb\" ))\n",
    "\n",
    "X_times_test = pickle.load(open(\"./dumps/X_times_test.p\", \"rb\" ))\n",
    "X_sentence_len_test = pickle.load(open(\"./dumps/X_sentence_len_test.p\", \"rb\" ))\n",
    "X_post_cnt_test = pickle.load(open(\"./dumps/X_post_cnt_test.p\", \"rb\" ))\n",
    "X_sentiment_test = pickle.load(open(\"./dumps/X_sentiment_test.p\", \"rb\" ))\n",
    "X_subjectivity_test = pickle.load(open(\"./dumps/X_subjectivity_test.p\", \"rb\" ))\n",
    "\n",
    "X_pos_tags_train = pickle.load(open( \"X_pos_tags_train.p\", \"rb\" ))\n",
    "X_pos_tags_test = pickle.load(open( \"X_pos_tags_test.p\", \"rb\" ))\n",
    "\n",
    "X_lexicon_sizes_train = pickle.load(open( \"X_lexicon_sizes_train.p\", \"rb\" ))\n",
    "X_lexicon_sizes_test = pickle.load(open( \"X_lexicon_sizes_test.p\", \"rb\" ))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for Corpus preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_diff = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'he', 'him', 'his', 'himself', \n",
    "           'she', 'her', 'hers', 'herself', 'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', \n",
    "           'themselves', 'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "           'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', 'totally', 'whole',\n",
    "           'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.stopwords.difference_update(sw_diff)\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X, method='lem'):\n",
    "        return [\n",
    "            list(self.tokenize(doc, method)) for doc in X\n",
    "        ]   \n",
    "\n",
    "    def tokenize(self, document, method='lem'):\n",
    "        if(method == 'lem'):\n",
    "            # Break the document into sentences\n",
    "            for sent in sent_tokenize(document):\n",
    "                # Break the sentence into part of speech tagged tokens\n",
    "                for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "                    # Apply preprocessing to the token\n",
    "                    token = self.process_token(token)\n",
    "                    if not self.is_valid_token(token):\n",
    "                        continue\n",
    "                        \n",
    "                    # Lemmatize the token and yield\n",
    "                    lemma = self.lemmatize(token, tag)\n",
    "                    yield lemma\n",
    "                    \n",
    "        elif(method == 'stem'):\n",
    "            # Break the document into tokens\n",
    "            for token in wordpunct_tokenize(document):\n",
    "                # Apply preprocessing to the token\n",
    "                token = self.process_token(token)\n",
    "                if not self.is_valid_token(token):\n",
    "                    continue\n",
    "                \n",
    "                stem = self.stem(token)\n",
    "                yield stem\n",
    "        else:\n",
    "            raise ValueError('Unknown method type.')\n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)\n",
    "    \n",
    "    def stem(self, token):\n",
    "        return self.stemmer.stem(token)\n",
    "    \n",
    "    def process_token(self, token):\n",
    "        token = token.lower() if self.lower else token\n",
    "        token = token.strip() if self.strip else tcharoken\n",
    "        token = token.strip('_') if self.strip else token\n",
    "        token = token.strip('*') if self.strip else token\n",
    "        return token\n",
    "    \n",
    "    def is_valid_token(self, token):\n",
    "        # If stopword, token is invalid\n",
    "        if token in self.stopwords:\n",
    "            return False\n",
    "\n",
    "        # If punctuation, token is invalid\n",
    "        if all(char in self.punct for char in token):\n",
    "            return False\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code loads data corpus from multiple files into lists X (texts) and y(labels) with one entry per user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_entries(X, y, path_list, label_dict=None, default_label=0):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "    \n",
    "    IMAGE_STR = 'data:image'\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "        \n",
    "            user_text = ''\n",
    "            for post in root.findall('.//TITLE') + root.findall('.//TEXT'):\n",
    "                post = post.text.strip().strip()\n",
    "                if post != '':\n",
    "                    if IMAGE_STR in post:\n",
    "                        continue\n",
    "                    post = re.sub(r\"http\\S+\", \" \", post)\n",
    "                    post = re.sub(r\"\\d+\", \" \", post)\n",
    "                    post = re.sub(u\"\\xa0\", \" \", post)\n",
    "                    post = re.sub(u\"\\\\p{P}+\", \" \", post)\n",
    "                    user_text += ' ' + post.lower()\n",
    "            \n",
    "            X.append(user_text)\n",
    "            label = int(label_dict[user_id]) if label_dict else default_label\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility methods for extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sentence_length(sentences):\n",
    "    sum = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.replace(' ', '')\n",
    "        sum += len(sentence)\n",
    "    return sum / len(sentences) if sentences else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_and_subjectivity(sentences):\n",
    "    sum_sentiment = 0\n",
    "    sum_subjectivity = 0\n",
    "    if len(sentences) > 0:\n",
    "        for sentence in sentences:\n",
    "            tb = TextBlob(sentence)\n",
    "            sum_sentiment += tb.sentiment.polarity\n",
    "            sum_subjectivity += tb.sentiment.subjectivity\n",
    "        sum_sentiment = sum_sentiment / float(len(sentences))\n",
    "        sum_subjectivity = sum_subjectivity / float(len(sentences))\n",
    "        return (sum_sentiment, sum_subjectivity)\n",
    "    else:\n",
    "        return (0.0, 0.0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_features(X_times, X_sentence_lengths, X_post_cnt, X_sentiment,\n",
    "                  X_subjectivity, X_post_lengths, X_post_freq, path_list):\n",
    "    entry_lists = []\n",
    "    for path in path_list:\n",
    "        entry_lists.append(os.scandir(path))\n",
    "        \n",
    "    IMAGE_STR = 'data:image'\n",
    "    datetime_pattern = '%Y-%m-%d %H:%M:%S'\n",
    "    date_end = None\n",
    "    date_start = None\n",
    "    \n",
    "    for list_of_entries in entry_lists:\n",
    "        for entry in list_of_entries:\n",
    "            root = etree.parse(entry.path).getroot()\n",
    "            user_id = root[0].text\n",
    "            \n",
    "            sentences = []\n",
    "            post_lengths = []\n",
    "            post_cnt = 0\n",
    "            for post in root.findall('.//TEXT'):\n",
    "                post_cnt += 1\n",
    "                post = post.text.strip()\n",
    "                if post != '':\n",
    "                    sentences.extend(sent_tokenize(post))\n",
    "                    post_lengths.append(len(post))\n",
    "                else:\n",
    "                    post_lengths.append(0)\n",
    "            \n",
    "            avg_sentiment, avg_subjectivity = get_sentiment_and_subjectivity(sentences)\n",
    "            avg_sentence_length = get_avg_sentence_length(sentences)\n",
    "            avg_post_length = np.mean(post_lengths)\n",
    "            \n",
    "            sum_hours = 0\n",
    "            for date in root.findall('.//DATE'):\n",
    "                date = date.text.strip()\n",
    "                if date != '':                    \n",
    "                    if not date_end:\n",
    "                        date_end = datetime.strptime(date, datetime_pattern)\n",
    "                    date_start = datetime.strptime(date, datetime_pattern)\n",
    "                    \n",
    "                    m = re.match(r'\\d{4}-\\d{2}-\\d{2} (\\d{2}).*', date)\n",
    "                    hour = int(m.group(1))\n",
    "                    sum_hours += hour\n",
    "            \n",
    "            post_span_minutes = (date_end - date_start).total_seconds()/60\n",
    "            post_freq = post_span_minutes / post_cnt\n",
    "            \n",
    "            time = [0] * 8\n",
    "            avg_hour = sum_hours / post_cnt\n",
    "            index = int(avg_hour // 3)\n",
    "            time[index] = 1\n",
    "            \n",
    "            X_post_cnt.append([post_cnt])\n",
    "            X_sentence_lengths.append([avg_sentence_length])\n",
    "            X_times.append(time)\n",
    "            X_sentiment.append([avg_sentiment])\n",
    "            X_subjectivity.append([avg_subjectivity])\n",
    "            X_post_lengths.append([avg_post_length])\n",
    "            X_post_freq.append([post_freq])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading input files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "TRAIN_PATH = os.path.join(cwd, \"reddit-training-ready-to-share\")\n",
    "TEST_PATH = os.path.join(cwd, \"reddit-test-data-ready-to-share\")\n",
    "\n",
    "TRAIN_POSITIVE_PATH = os.path.join(TRAIN_PATH, \"positive_examples_anonymous\")\n",
    "TRAIN_NEGATIVE_PATH = os.path.join(TRAIN_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TEST_POSITIVE_PATH = os.path.join(TEST_PATH, \"positive_examples_anonymous\")\n",
    "TEST_NEGATIVE_PATH = os.path.join(TEST_PATH, \"negative_examples_anonymous\")\n",
    "\n",
    "TRAIN_LABELS_PATH = os.path.join(cwd, 'risk_golden_truth.txt')\n",
    "\n",
    "IMAGE_STR = 'data:image'\n",
    "\n",
    "train_labels_file = open(TRAIN_LABELS_PATH, 'r')\n",
    "train_label_dict = {}\n",
    "for line in train_labels_file:\n",
    "    xml_file, label = line.split(' ')\n",
    "    train_label_dict[xml_file] = label\n",
    "train_labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = []\n",
    "y_train = []\n",
    "X_test_raw = []\n",
    "y_test = []\n",
    "\n",
    "train_entry_path_list = [TRAIN_POSITIVE_PATH, TRAIN_NEGATIVE_PATH]\n",
    "test_pos_entry_path_list = [TEST_POSITIVE_PATH]\n",
    "test_neg_entry_path_list = [TEST_NEGATIVE_PATH]\n",
    "\n",
    "read_entries(X=X_train_raw, y=y_train, path_list=train_entry_path_list, label_dict=train_label_dict)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_pos_entry_path_list, default_label=1)\n",
    "read_entries(X=X_test_raw, y=y_test, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_and_absolutizm_features(X_raw):\n",
    "    \n",
    "    fp_pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    \n",
    "    tp_pronouns = {'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "    \n",
    "    absolutisms = {'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "                   'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', \n",
    "                   'totally', 'whole', 'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "\n",
    "    fp_freq = []\n",
    "    tp_freq = []\n",
    "    absolutisms_freq = []\n",
    "    \n",
    "    for entry in X_raw:\n",
    "        sum_fp = 0\n",
    "        sum_tp = 0\n",
    "        sum_abs = 0\n",
    "        tokens = word_tokenize(entry)\n",
    "        for word in tokens:\n",
    "            if word in fp_pronouns:\n",
    "                sum_fp += 1\n",
    "            elif word in tp_pronouns:\n",
    "                sum_tp += 1\n",
    "            elif word in absolutisms:\n",
    "                sum_abs += 1\n",
    "        sum_fp = sum_fp / float(len(tokens))\n",
    "        sum_tp = sum_tp / float(len(tokens))\n",
    "        sum_abs = sum_abs / float(len(tokens))\n",
    "        fp_freq.append([sum_fp])\n",
    "        tp_freq.append([sum_tp])\n",
    "        absolutisms_freq.append([sum_abs])\n",
    "    \n",
    "    return (fp_freq, tp_freq, absolutisms_freq)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronoun_and_absolutizm_features2(X_raw):\n",
    "    \n",
    "    fp_pronouns = {'i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves'}\n",
    "    \n",
    "    tp_pronouns = {'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "                   'you', 'your', 'yours', 'yourselves', 'they', 'them', 'their', 'theirs', 'themselves'}\n",
    "    \n",
    "    absolutisms = {'absolutely', 'all', 'always', 'complete', 'completely', 'constant', 'constantly','definitely', \n",
    "                   'entire', 'ever', 'every', 'everyone', 'everything', 'full', 'must', 'never', 'nothing', \n",
    "                   'totally', 'whole', 'just', 'only', 'noone', 'none', 'no', 'nobody', 'each', 'everybody'}\n",
    "\n",
    "\n",
    "    fp_freq = []\n",
    "    tp_freq = []\n",
    "    absolutisms_freq = []\n",
    "    \n",
    "    for entry in X_raw:\n",
    "        sum_fp = 0\n",
    "        sum_tp = 0\n",
    "        sum_abs = 0\n",
    "        tokens = word_tokenize(entry)\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        tokens = [word for word in tokens if not word.isnumeric()]\n",
    "        fdist = FreqDist(tokens)\n",
    "        w, f = fdist.most_common(1)[0]\n",
    "        for word in tokens:\n",
    "            if word in fp_pronouns:\n",
    "                sum_fp += 1\n",
    "            elif word in tp_pronouns:\n",
    "                sum_tp += 1\n",
    "            elif word in absolutisms:\n",
    "                sum_abs += 1\n",
    "        sum_fp = sum_fp / float(f)\n",
    "        sum_tp = sum_tp / float(f)\n",
    "        sum_abs = sum_abs / float(f)\n",
    "        fp_freq.append([sum_fp])\n",
    "        tp_freq.append([sum_tp])\n",
    "        absolutisms_freq.append([sum_abs])\n",
    "    \n",
    "    return (fp_freq, tp_freq, absolutisms_freq)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_times_train = []\n",
    "X_times_test = []\n",
    "\n",
    "X_sentence_len_train = []\n",
    "X_sentence_len_test = []\n",
    "\n",
    "X_post_cnt_train = []\n",
    "X_post_cnt_test = []\n",
    "\n",
    "X_sentiment_train = []\n",
    "X_sentiment_test = []\n",
    "\n",
    "X_subjectivity_train = []\n",
    "X_subjectivity_test = []\n",
    "\n",
    "X_post_lengths_train = []\n",
    "X_post_lengths_test = []\n",
    "\n",
    "X_post_freq_train = []\n",
    "X_post_freq_test = []\n",
    "\n",
    "read_features(X_times=X_times_train, X_sentence_lengths=X_sentence_len_train, X_post_cnt=X_post_cnt_train,\n",
    "              X_sentiment=X_sentiment_train, X_subjectivity=X_subjectivity_train, X_post_lengths=X_post_lengths_train,\n",
    "              X_post_freq=X_post_freq_train, path_list=train_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_pos_entry_path_list)\n",
    "read_features(X_times=X_times_test, X_sentence_lengths=X_sentence_len_test, X_post_cnt=X_post_cnt_test,\n",
    "              X_sentiment=X_sentiment_test, X_subjectivity=X_subjectivity_test, X_post_lengths=X_post_lengths_test,\n",
    "              X_post_freq=X_post_freq_test, path_list=test_neg_entry_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fp_pronouns_train, X_tp_pronouns_train, X_absolutisms_train = get_pronoun_and_absolutizm_features2(X_train_raw)\n",
    "X_fp_pronouns_test, X_tp_pronouns_test, X_absolutisms_test = get_pronoun_and_absolutizm_features2(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_features(X):\n",
    "    lexicon = Empath()\n",
    "    \n",
    "    relevant_lexical_categories = ['negative_emotion', 'positive_emotion', 'communication',\n",
    "                                    'violence', 'business', 'nervousness', 'body', 'pain',\n",
    "                                    'internet', 'work', 'shame', 'poor'\n",
    "                              ]\n",
    "    \n",
    "    relevant_lexical_categories2 = ['negative_emotion', 'positive_emotion',\n",
    "                                   'nervousness', 'love', 'shame', 'pain'\n",
    "                              ]\n",
    "    \n",
    "    relevant_lexical_categories3 = ['friends', 'positive_emotion', 'negative_emotion', \n",
    "                                   'nervousness', 'love', 'shame', 'pain', 'optimism', 'sadness',\n",
    "                                    'speaking'\n",
    "                              ]\n",
    "    \n",
    "    feature_mat = []\n",
    "    for text in X:\n",
    "        d = lexicon.analyze(text, categories=relevant_lexical_categories3, normalize=True)\n",
    "        feature_mat.append([d[key] for key in sorted(d.keys(), reverse=False)])\n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sem_feat_train = get_semantic_features(X_train_raw)\n",
    "X_sem_feat_test = get_semantic_features(X_test_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_times_train, open(\"./dumps/X_times_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_train, open(\"./dumps/X_sentence_len_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_train, open(\"./dumps/X_post_cnt_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_train, open(\"./dumps/X_sentiment_train.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_train, open(\"./dumps/X_subjectivity_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_train, open(\"./dumps/X_post_lengths_train.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_train, open(\"./dumps/X_post_freq_train.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_train, open(\"./dumps/X_fp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_train, open(\"./dumps/X_tp_pronouns_train.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_train, open(\"./dumps/X_absolutisms_train.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_times_test, open(\"./dumps/X_times_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentence_len_test, open(\"./dumps/X_sentence_len_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_cnt_test, open(\"./dumps/X_post_cnt_test.p\", \"wb\" ))\n",
    "pickle.dump(X_sentiment_test, open(\"./dumps/X_sentiment_test.p\", \"wb\" ))\n",
    "pickle.dump(X_subjectivity_test, open(\"./dumps/X_subjectivity_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_lengths_test, open(\"./dumps/X_post_lengths_test.p\", \"wb\" ))\n",
    "pickle.dump(X_post_freq_test, open(\"./dumps/X_post_freq_test.p\", \"wb\" ))\n",
    "pickle.dump(X_fp_pronouns_test, open(\"./dumps/X_fp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_tp_pronouns_test, open(\"./dumps/X_tp_pronouns_test.p\", \"wb\" ))\n",
    "pickle.dump(X_absolutisms_test, open(\"./dumps/X_absolutisms_test.p\", \"wb\" ))\n",
    "\n",
    "pickle.dump(X_sem_feat_train, open(\"./dumps/X_sem_feat_train.p\", \"wb\" ))\n",
    "pickle.dump(X_sem_feat_test, open(\"./dumps/X_sem_feat_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use X list as input to NLTKPreprocessor class which outputs list of preprocessed, tokenized texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "X_train_prep = preprocessor.transform(X_train_raw, method=preprocess_method)\n",
    "X_test_prep = preprocessor.transform(X_test_raw, method=preprocess_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tags(X):\n",
    "    pos_tag_mat = []\n",
    "    for tokens in X:\n",
    "       # tag_dict = { 'CC': 0, 'DT': 0, 'IN': 0, 'JJ': 0, 'JJR': 0, 'JJS': 0,\n",
    "       #             'NN': 0, 'NNP':0, 'NNS': 0, 'PRP': 0, 'PRP$': 0, 'RB': 0,\n",
    "       #             'RBR': 0, 'RBS': 0, 'RP': 0, 'VB': 0, 'VBD': 0, 'VBG': 0,\n",
    "        #            'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        tag_dict = {'JJ': 0, 'JJS': 0,\n",
    "                    'NN': 0, 'NNS': 0, 'PRP': 0, 'PRP$': 0,\n",
    "                    'RBS': 0, 'VB': 0, 'VBD': 0,\n",
    "                    'VBN': 0, 'VBP': 0, 'VBZ': 0}\n",
    "        \n",
    "        text_len = len(tokens)\n",
    "        tags = pos_tag(tokens)\n",
    "        \n",
    "        for word, tag in tags:\n",
    "            if tag in tag_dict.keys():\n",
    "                tag_dict[tag] += 1/text_len\n",
    "        \n",
    "        tag_freq = [tag_dict[key] for key in sorted(tag_dict.keys(), reverse=False)]\n",
    "        pos_tag_mat.append(tag_freq)\n",
    "    \n",
    "    return pos_tag_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos_tags_train = get_pos_tags(X_train_prep)\n",
    "X_pos_tags_test = get_pos_tags(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_pos_tags_train, open(\"./dumps/X_pos_tags_train.p\", \"wb\" ))\n",
    "pickle.dump(X_pos_tags_test, open(\"./dumps/X_pos_tags_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_sizes(X):\n",
    "    unique_cnt_mat = []\n",
    "    for tokens in X:\n",
    "        unique_cnt_mat.append([len(set(tokens))])\n",
    "    \n",
    "    return unique_cnt_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lexicon_sizes_train = get_lexicon_sizes(X_train_prep)\n",
    "X_lexicon_sizes_test = get_lexicon_sizes(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_lexicon_sizes_train, open(\"./dumps/X_lexicon_sizes_train.p\", \"wb\" ))\n",
    "pickle.dump(X_lexicon_sizes_test, open(\"./dumps/X_lexicon_sizes_test.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use tf-idf vectorizer for vector representation of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(arg):\n",
    "    \"\"\"\n",
    "    Simple identity function works as a passthrough.\n",
    "    \"\"\"\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 1), min_df=30)\n",
    "X_train = vect.fit_transform(X_train_prep, y_train)\n",
    "X_test = vect.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vect.get_feature_names())\n",
    "#print(X_train.getnnz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing and adding features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_abs_scaler = StandardScaler()\n",
    "#max_abs_scaler = MaxAbsScaler()\n",
    "#max_abs_scaler = MinMaxScaler()\n",
    "X_sentence_len_train_scaled = max_abs_scaler.fit_transform(X_sentence_len_train)\n",
    "X_sentence_len_test_scaled = max_abs_scaler.transform(X_sentence_len_test)\n",
    "\n",
    "X_sem_feat_train_scaled = max_abs_scaler.fit_transform(X_sem_feat_train)\n",
    "X_sem_feat_test_scaled = max_abs_scaler.transform(X_sem_feat_test)\n",
    "\n",
    "X_post_cnt_train_scaled = max_abs_scaler.fit_transform(X_post_cnt_train)\n",
    "X_post_cnt_test_scaled = max_abs_scaler.transform(X_post_cnt_test)\n",
    "\n",
    "X_sentiment_train_scaled = max_abs_scaler.fit_transform(X_sentiment_train)\n",
    "X_sentiment_test_scaled = max_abs_scaler.transform(X_sentiment_test)\n",
    "\n",
    "X_subjectivity_train_scaled = max_abs_scaler.fit_transform(X_subjectivity_train)\n",
    "X_subjectivity_test_scaled = max_abs_scaler.transform(X_subjectivity_test)\n",
    "\n",
    "X_fp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_fp_pronouns_train)\n",
    "X_fp_pronouns_test_scaled = max_abs_scaler.transform(X_fp_pronouns_test)\n",
    "\n",
    "X_tp_pronouns_train_scaled = max_abs_scaler.fit_transform(X_tp_pronouns_train)\n",
    "X_tp_pronouns_test_scaled = max_abs_scaler.transform(X_tp_pronouns_test)\n",
    "\n",
    "X_absolutisms_train_scaled = max_abs_scaler.fit_transform(X_absolutisms_train)\n",
    "X_absolutisms_test_scaled = max_abs_scaler.transform(X_absolutisms_test)\n",
    "\n",
    "X_pos_tags_train_scaled = max_abs_scaler.fit_transform(X_pos_tags_train)\n",
    "X_pos_tags_test_scaled = max_abs_scaler.transform(X_pos_tags_test)\n",
    "\n",
    "X_lexicon_sizes_train_scaled = max_abs_scaler.fit_transform(X_lexicon_sizes_train)\n",
    "X_lexicon_sizes_test_scaled = max_abs_scaler.transform(X_lexicon_sizes_test)\n",
    "\n",
    "X_post_lengths_train_scaled = max_abs_scaler.fit_transform(X_post_lengths_train)\n",
    "X_post_lengths_test_scaled = max_abs_scaler.transform(X_post_lengths_test)\n",
    "\n",
    "X_post_freq_train_scaled = max_abs_scaler.fit_transform(X_post_freq_train)\n",
    "X_post_freq_test_scaled = max_abs_scaler.transform(X_post_freq_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building and evaluating models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to find allsubsets of a set (needed to find all subsets of set of new feature for CV)\n",
    "allsubsets = lambda n: list(chain(*[combinations(range(n), ni) for ni in range(n+1)]))\n",
    "\n",
    "# r - return only subsets of r size (reduction of search space) + empty set; if None, return all subsets\n",
    "def get_subsets(n, r=None):\n",
    "    if r==None:\n",
    "        return allsubsets(n)\n",
    "    else:\n",
    "        combs = list(combinations(range(n), r))\n",
    "        return combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_weight(w):\n",
    "    return 1.0 / (1.0 + w)\n",
    "\n",
    "def minority_weight(w):\n",
    "    return w / (1.0 + w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(get_subsets(12, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for k-fold cross-validation of a model\n",
    "#Performs grid search on C parameter, subsets of new features and k-best token features by chi2 stat\n",
    "#For model_name parameter use 'SVM' - SVM or 'LR' - LogisticRegression\n",
    "\n",
    "#EXAMPLE: see example below baseline for CV of SVM model\n",
    "def cross_validate_model(model_name, X_train, y_train, C_list, k_list, new_features, k_folds, subset_size=None,\n",
    "                        regularization='l2'):\n",
    "    feature_subsets = get_subsets(len(new_features), subset_size)\n",
    "    best_C = 0\n",
    "    best_k = 0\n",
    "    best_feature_set = {}\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        \n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        for c in C_list:\n",
    "            \n",
    "            if model_name == 'SVM':\n",
    "                model = LinearSVC(class_weight='balanced', C=c, penalty=regularization) \n",
    "            elif model_name == 'LR':\n",
    "                model = LogisticRegression(class_weight='balanced', C=c, penalty=regularization)\n",
    "            \n",
    "            for subset in feature_subsets:\n",
    "                X_new = X_kbest_train\n",
    "                for i in subset:\n",
    "                    X_new = hstack([X_new, new_features[i]])\n",
    "                scores = cross_val_score(model, X_new, y_train, cv=k_folds, scoring=scorer)\n",
    "                mean = scores.mean()\n",
    "                if mean > best_score:\n",
    "                    best_score = mean\n",
    "                    best_C = c\n",
    "                    best_k = k_features\n",
    "                    best_feature_set = subset\n",
    "         \n",
    "    return (best_score, best_k, best_C, best_feature_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_cross_validate_model(model_name, X_train, y_train, C_list, k_list, k_folds, feature, regularization='l2'):\n",
    "    best_C = 0\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])    \n",
    "    best_k = 0\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        X_kbest_train = hstack([X_kbest_train, feature])\n",
    "\n",
    "        for c in C_list:\n",
    "                if model_name == 'SVM':\n",
    "                    model = LinearSVC(class_weight='balanced', C=c, penalty=regularization) \n",
    "                elif model_name == 'LR':\n",
    "                    model = LogisticRegression(class_weight='balanced', C=c, penalty=regularization)\n",
    "\n",
    "                scores = cross_val_score(model, X_kbest_train, y_train, cv=k_folds, scoring=scorer)\n",
    "                mean = scores.mean()\n",
    "                if mean > best_score:\n",
    "                        best_score = mean\n",
    "                        best_C = c\n",
    "                        best_k = k_features\n",
    "\n",
    "    return (best_score, best_C, best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline(model_name, X_train, y_train, C_list, k_list, k_folds, regularization='l2'):\n",
    "    best_C = 0\n",
    "    best_score = -1\n",
    "    best_k = 0\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "    \n",
    "    for k_features in k_list:\n",
    "        \n",
    "        chi2_selector = SelectKBest(chi2, k=k_features)\n",
    "        X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "        \n",
    "        for c in C_list:\n",
    "            if model_name == 'SVM':\n",
    "                model = LinearSVC(class_weight=\"balanced\", C=c, penalty=regularization) \n",
    "            elif model_name == 'LR':\n",
    "                model = LogisticRegression(class_weight=\"balanced\", C=c, penalty=regularization)\n",
    "\n",
    "            scores = cross_val_score(model, X_kbest_train, y_train, cv=k_folds, scoring=scorer)\n",
    "            mean = scores.mean()\n",
    "            if mean > best_score:\n",
    "                    best_score = mean\n",
    "                    best_C = c\n",
    "                    best_k = k_features\n",
    "         \n",
    "    return (best_score, best_k, best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_new_features(model_name, X_train, y_train, C_list, k_folds, regularization='l2'):\n",
    "    best_C = 0\n",
    "    best_score = -1\n",
    "    scorer = make_scorer(f1_score, average='macro', labels=[1])\n",
    "        \n",
    "    for c in C_list:\n",
    "        if model_name == 'SVM':\n",
    "            model = LinearSVC(class_weight=\"balanced\", C=c, penalty=regularization) \n",
    "        elif model_name == 'LR':\n",
    "            model = LogisticRegression(class_weight=\"balanced\", C=c, penalty=regularization)\n",
    "\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=k_folds, scoring=scorer)\n",
    "        mean = scores.mean()\n",
    "        if mean > best_score:\n",
    "                best_score = mean\n",
    "                best_C = c\n",
    "         \n",
    "    return (best_score, best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6671353496353496\n",
      "Best C: 16.0\n",
      "Best k: 1500\n"
     ]
    }
   ],
   "source": [
    "#CV for Logistic Regression Baseline\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "k_range = [1000, 1500, 'all']\n",
    "\n",
    "best_score, best_k, best_C = cross_validate_baseline('LR', X_train, y_train, C_range, k_range, 5, 'l1')\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6879221432162609\n",
      "Best C: 8.0\n",
      "Best k: 1500\n"
     ]
    }
   ],
   "source": [
    "#CV for SVM baseline\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "k_range = [1000, 1500, 'all']\n",
    "\n",
    "best_score, best_k, best_C = cross_validate_baseline('SVM', X_train, y_train, C_range, k_range, 5)\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: BASELINE Logistic Regression L1\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9517    0.8949    0.9224       352\n",
      "          1     0.5067    0.7037    0.5891        54\n",
      "\n",
      "avg / total     0.8925    0.8695    0.8781       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: BASELINE Logistic Regression L1\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', penalty='l1', C=16.0)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "model.fit(X_kbest_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_kbest_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: BASELINE SVM\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9547    0.8977    0.9253       352\n",
      "          1     0.5200    0.7222    0.6047        54\n",
      "\n",
      "avg / total     0.8969    0.8744    0.8827       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: BASELINE SVM\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced', C=8.0)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "model.fit(X_kbest_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_kbest_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting classifier for baselines (SVM + LR)\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9452    0.9318    0.9385       352\n",
      "          1     0.5932    0.6481    0.6195        54\n",
      "\n",
      "avg / total     0.8984    0.8941    0.8961       406\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrija/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(\"Voting classifier for baselines (SVM + LR)\")\n",
    "\n",
    "model1 = LogisticRegression(class_weight='balanced', C=16.0, penalty='l1')\n",
    "model2 = SVC(class_weight='balanced', kernel='linear', C=8.0, probability=True)\n",
    "model = VotingClassifier(estimators=[('lr', model1), ('svm', model2)], voting='soft')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "model.fit(X_kbest_train, y_train)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "y_pred = model.predict(X_kbest_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.7338615522783135\n",
      "Best C: 32.0\n",
      "Best k: 1200\n",
      "Best new feature subset: (0, 1, 2, 3, 4, 5, 6, 7, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "#Cross validation of SVM\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "#k_range = np.arange(10, 3000, 300)\n",
    "#C_range = [0.01, 0.1, 1, 10, 100]\n",
    "k_range = [1500]\n",
    "new_features = [X_times_train, X_sentence_len_train_scaled, X_sem_feat_train_scaled,\n",
    "                      X_post_cnt_train_scaled, X_sentiment_train_scaled, X_subjectivity_train_scaled, \n",
    "                      X_fp_pronouns_train_scaled, X_tp_pronouns_train_scaled, X_absolutisms_train_scaled, \n",
    "                      X_pos_tags_train_scaled, X_lexicon_sizes_train_scaled, X_post_lengths_train_scaled, \n",
    "                      X_post_freq_train_scaled]\n",
    "\n",
    "#best_feature_set vraća kao tuple indexa u listi new_features (gore)\n",
    "best_score, best_k, best_C, best_feature_set = cross_validate_model('SVM', X_train, y_train, C_range,\n",
    "                                                                   k_range, new_features, 4, 10)\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))\n",
    "print(\"Best new feature subset: \" + str(best_feature_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training = [X_times_train, X_sentence_len_train_scaled, X_sem_feat_train_scaled,\n",
    "                      X_post_cnt_train_scaled, X_sentiment_train_scaled, X_subjectivity_train_scaled, \n",
    "                      X_fp_pronouns_train_scaled, X_tp_pronouns_train_scaled, X_absolutisms_train_scaled, \n",
    "                      X_pos_tags_train_scaled, X_lexicon_sizes_train_scaled, X_post_lengths_train_scaled, \n",
    "                      X_post_freq_train_scaled]\n",
    "\n",
    "final_testing = [X_times_test, X_sentence_len_test_scaled, X_sem_feat_test_scaled,\n",
    "                      X_post_cnt_test_scaled, X_sentiment_test_scaled, X_subjectivity_test_scaled, \n",
    "                      X_fp_pronouns_test_scaled, X_tp_pronouns_test_scaled, X_absolutisms_test_scaled, \n",
    "                      X_pos_tags_test_scaled, X_lexicon_sizes_test_scaled, X_post_lengths_test_scaled, \n",
    "                      X_post_freq_test_scaled]\n",
    "\n",
    "reduced_training = [X_sentence_len_train_scaled, X_sem_feat_train_scaled[:,:3],\n",
    "                      X_sentiment_train_scaled, X_subjectivity_train_scaled, \n",
    "                      X_fp_pronouns_train_scaled, X_tp_pronouns_train_scaled, X_absolutisms_train_scaled, \n",
    "                      X_lexicon_sizes_train_scaled, X_post_lengths_train_scaled, \n",
    "                      X_post_freq_train_scaled]\n",
    "\n",
    "reduced_testing = [X_sentence_len_test_scaled, X_sem_feat_test_scaled[:,:3],\n",
    "                    X_sentiment_test_scaled, X_subjectivity_test_scaled, \n",
    "                      X_fp_pronouns_test_scaled, X_tp_pronouns_test_scaled, X_absolutisms_test_scaled, \n",
    "                      X_lexicon_sizes_test_scaled, X_post_lengths_test_scaled, \n",
    "                      X_post_freq_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6091847684072148\n",
      "Best C: 0.5\n"
     ]
    }
   ],
   "source": [
    "#malo igranje\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "k_range = [1000, 1500, 'all']\n",
    "\n",
    "best_score, best_C = cross_validate_new_features('LR', np.hstack(final_training), y_train, C_range, 5, regularization='l1')\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features : 10\n",
      "[ True False  True  True False  True  True  True  True  True  True  True]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafdbdbf198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "estimator = LogisticRegression(class_weight='balanced', C=1.0, penalty='l1')\n",
    "selector = RFECV(estimator, step=1, cv=StratifiedKFold(5), scoring=make_scorer(f1_score, average='macro', labels=[1]))\n",
    "X_train_new = selector.fit_transform(np.hstack(reduced_training), y_train)\n",
    "X_test_new = selector.transform(np.hstack(reduced_testing))\n",
    "print(\"Optimal number of features : %d\" % selector.n_features_)\n",
    "print(selector.support_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (F1 score)\")\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 6 (0.140747)\n",
      "2. feature 2 (0.096533)\n",
      "3. feature 10 (0.092257)\n",
      "4. feature 3 (0.091629)\n",
      "5. feature 8 (0.089495)\n",
      "6. feature 11 (0.081412)\n",
      "7. feature 7 (0.075232)\n",
      "8. feature 5 (0.073490)\n",
      "9. feature 1 (0.072865)\n",
      "10. feature 9 (0.065174)\n",
      "11. feature 0 (0.063258)\n",
      "12. feature 4 (0.057908)\n",
      "(486, 5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fafdbaf3da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=500, random_state=0)\n",
    "X = np.hstack(reduced_training)\n",
    "forest.fit(X, y_train)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "    \n",
    "selector = SelectFromModel(forest, prefit=True)\n",
    "X_train_new = selector.transform(np.hstack(reduced_training))\n",
    "X_test_new = selector.transform(np.hstack(reduced_testing))\n",
    "print(X_train_new.shape)\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6635894660894661\n",
      "Best C: 22\n",
      "Best k: 1500\n"
     ]
    }
   ],
   "source": [
    "#Individual feature CV\n",
    "#C_range = 2. ** np.arange(-1, 8)\n",
    "C_range = [1, 2, 4, 8, 9, 10, 12, 14, 16, 18, 20, 21, 22, 23, 32, 64]\n",
    "k_range = [1500]\n",
    "best_score, best_C, best_k = feature_cross_validate_model('LR', X_train, y_train, C_range, k_range, 5, \n",
    "                                                  X_train_new, regularization='l1')\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: Logistic Regression classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9520    0.9006    0.9255       352\n",
      "          1     0.5205    0.7037    0.5984        54\n",
      "\n",
      "avg / total     0.8946    0.8744    0.8820       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: Logistic Regression classifier\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', C=22.0, penalty='l1')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = hstack([X_kbest_train, X_train_new])  \n",
    "X_final_test = hstack([X_kbest_test, X_test_new])\n",
    "\n",
    "#X_final_train = np.hstack(final_training)\n",
    "#X_final_test = np.hstack(final_testing)\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: SVM classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9548    0.9006    0.9269       352\n",
      "          1     0.5270    0.7222    0.6094        54\n",
      "\n",
      "avg / total     0.8979    0.8768    0.8847       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: SVM classifier\")\n",
    "\n",
    "model = LinearSVC(class_weight='balanced', C=8.0)\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = hstack([X_kbest_train, X_train_new])  \n",
    "X_final_test = hstack([X_kbest_test, X_test_new])\n",
    "#X_final_train = np.hstack(final_training)\n",
    "#X_final_test = np.hstack(final_testing)\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6428862411297744\n",
      "Best C: 32.0\n",
      "Best k: 1500\n",
      "Best new feature subset: (0, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "#Cross validation of Logistic Regression BEST FEATURES\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "k_range = [1500, 'all']\n",
    "new_features = [final_training[2][:,:3], final_training[5], final_training[7]]\n",
    "\n",
    "#best_feature_set vraća kao tuple indexa u listi new_features (gore)\n",
    "best_score, best_k, best_C, best_feature_set_lr = cross_validate_model('LR', X_train, y_train, C_range,\n",
    "                                                                   k_range, new_features, 5, 3, 'l1')\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))\n",
    "print(\"Best new feature subset: \" + str(best_feature_set_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.6747660427807486\n",
      "Best C: 4.0\n",
      "Best k: 1500\n",
      "Best new feature subset: (0, 1, 2, 3, 5, 6, 7, 8, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "#Crossvalidation for Logistic Regression - INDEPENDENT of SVM\n",
    "C_range = 2. ** np.arange(-1, 9)\n",
    "#k_range = np.arange(10, 3000, 300)\n",
    "#C_range = [0.01, 0.1, 1, 10, 100]\n",
    "k_range = [1200, 1500, 1700]\n",
    "new_features_lr = [X_times_train, X_sem_feat_train_scaled,\n",
    "                      X_post_cnt_train_scaled, X_sentiment_train_scaled, X_subjectivity_train_scaled, \n",
    "                      X_fp_pronouns_train_scaled, X_tp_pronouns_train_scaled, X_absolutisms_train_scaled, \n",
    "                      X_pos_tags_train_scaled, X_lexicon_sizes_train_scaled, X_post_lengths_train_scaled, \n",
    "                      X_post_freq_train_scaled]\n",
    "\n",
    "#best_feature_set vraća kao tuple indexa u listi new_features (gore)\n",
    "best_score, best_k, best_C, best_feature_set_lr = cross_validate_model('LR', X_train, y_train, C_range,\n",
    "                                                                   k_range, new_features_lr, 4, 10, 'l1')\n",
    "\n",
    "print(\"Best score: \" + str(best_score))\n",
    "print(\"Best C: \" + str(best_C))\n",
    "print(\"Best k: \" + str(best_k))\n",
    "print(\"Best new feature subset: \" + str(best_feature_set_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_test_lr = [X_times_test, X_sem_feat_test_scaled,\n",
    "                      X_post_cnt_test_scaled, X_sentiment_test_scaled, X_subjectivity_test_scaled, \n",
    "                      X_fp_pronouns_test_scaled, X_tp_pronouns_test_scaled, X_absolutisms_test_scaled, \n",
    "                      X_lexicon_sizes_test_scaled, X_post_lengths_test_scaled]\n",
    "\n",
    "new_features_test_lr_2 = [X_times_test, X_sem_feat_test_scaled,\n",
    "                      X_post_cnt_test_scaled, X_sentiment_test_scaled, X_subjectivity_test_scaled, \n",
    "                      X_fp_pronouns_test_scaled, X_tp_pronouns_test_scaled, X_absolutisms_test_scaled, \n",
    "                      X_pos_tags_test_scaled, X_lexicon_sizes_test_scaled, X_post_lengths_test_scaled, \n",
    "                      X_post_freq_test_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building for evaluation: Logistic Regression classifier\n",
      "Evaluation model fit\n",
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9567    0.8153    0.8804       352\n",
      "          1     0.3868    0.7593    0.5125        54\n",
      "\n",
      "avg / total     0.8809    0.8079    0.8314       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Building for evaluation: Logistic Regression classifier\")\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced', C=4.0, penalty='l1')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1500)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_test)\n",
    "\n",
    "X_final_train = X_kbest_train\n",
    "for i in best_feature_set_lr:\n",
    "    X_final_train = hstack([X_final_train, new_features_lr[i]])\n",
    "    \n",
    "X_final_test = X_kbest_test\n",
    "for i in best_feature_set_lr:\n",
    "    X_final_test = hstack([X_final_test, new_features_test_lr_2[i]])\n",
    "\n",
    "model.fit(X_final_train, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model.predict(X_final_test)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0     0.9537    0.8778    0.9142       352\n",
      "          1     0.4756    0.7222    0.5735        54\n",
      "\n",
      "avg / total     0.8901    0.8571    0.8689       406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(class_weight='balanced', C=4.0)\n",
    "lr = LogisticRegression(class_weight='balanced', C=64.0, penalty='l1')\n",
    "baseline = LinearSVC(class_weight='balanced')\n",
    "\n",
    "chi2_selector = SelectKBest(chi2, k=1200)\n",
    "X_kbest_train = chi2_selector.fit_transform(X_all_train, y_train)\n",
    "X_kbest_test = chi2_selector.transform(X_all_test)\n",
    "\n",
    "svm.fit(X_kbest_train, y_train)\n",
    "svm_pred = svm.predict(X_kbest_test)\n",
    "\n",
    "lr.fit(X_kbest_train, y_train)\n",
    "lr_pred = lr.predict(X_kbest_test)\n",
    "\n",
    "baseline.fit(X_train, y_train)\n",
    "bl_pred = baseline.predict(X_test)\n",
    "\n",
    "result = []\n",
    "for i in range(len(svm_pred)):\n",
    "    suma = svm_pred[i] + lr_pred[i] + bl_pred[i]\n",
    "    if suma < 2:\n",
    "        result.append(0)\n",
    "    else:\n",
    "        result.append(1)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the complete model on whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vstack((X_train_2, X_test_2))\n",
    "y = y_train + y_test\n",
    "\n",
    "model_complete = LinearSVC(class_weight='balanced')\n",
    "model_complete.fit(X, y)\n",
    "\n",
    "print(\"Complete model fit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most informative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(show_most_informative_features(vect, model_complete))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some baseline classifier testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(tokenizer=identity, preprocessor=None, lowercase=False, ngram_range=(1, 2), min_df=20)\n",
    "X_train_3 = vect2.fit_transform(X_train_prep, y_train)\n",
    "X_test_3 = vect2.transform(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: BernoulliNB classifier\")\n",
    "\n",
    "model2 = BernoulliNB()\n",
    "model2.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model2.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: MultinomialNB classifier\")\n",
    "\n",
    "model3 = MultinomialNB()\n",
    "model3.fit(X_train_3, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model3.predict(X_test_3)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: DecisionTree classifier\")\n",
    "\n",
    "model4 = DecisionTreeClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test_2)\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Building for evaluation: RandomForest classifier\")\n",
    "\n",
    "model4 = RandomForestClassifier(class_weight='balanced')\n",
    "model4.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(\"Evaluation model fit\")\n",
    "print(\"Classification Report:\\n\")\n",
    "\n",
    "y_pred = model4.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Empath testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexicon = Empath()\n",
    "relevant_lexical_categories1 = ['help', 'medical_emergency', 'hate', 'health', 'suffering', \n",
    "                               'kill', 'fear', 'death', 'violence', 'love',\n",
    "                               'anonymity', 'injury', 'appearance', 'sadness',\n",
    "                               'emotional', 'ugliness', 'shame', 'torment',\n",
    "                               'pain', 'negative_emotion', 'positive_emotion', 'friends',\n",
    "                               'alcohol', 'nervousness', 'optimism', 'body', 'contentment'\n",
    "                               'cold', 'school', 'communication', 'work', 'sleep', 'play'\n",
    "                               'trust', 'social_media', 'sexual'\n",
    "                              ]\n",
    "\n",
    "relevant_lexical_categories = ['negative_emotion', 'speaking', 'positive_emotion', 'communication',\n",
    "                               'friends', 'children', 'optimism', 'violence', 'pain', 'family',\n",
    "                               'trust', 'love', 'party', 'business', 'home', 'shame', 'listen',\n",
    "                               'giving', 'body', 'suffering', 'work', 'nervousness', 'strength',\n",
    "                               'hearing', 'health', 'traveling', 'wedding', 'childish', 'hate',\n",
    "                               'social_media', 'sadness', 'school'\n",
    "                              ]\n",
    "\n",
    "x_senti1 = []\n",
    "y_senti1 = []\n",
    "read_entries(X=x_senti1, y=y_senti1, path_list=test_pos_entry_path_list, default_label=1)\n",
    "\n",
    "x_senti2 = []\n",
    "y_senti2 = []\n",
    "read_entries(X=x_senti2, y=y_senti2, path_list=test_neg_entry_path_list, default_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dict1 = {}\n",
    "len1 = len(x_senti1)\n",
    "for i in x_senti1:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict1 = { k: d.get(k, 0)/len1 + avg_dict1.get(k, 0) for k in set(d) | set(avg_dict1) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict1.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_dict2 = {}\n",
    "len2 = len(x_senti2)\n",
    "for i in x_senti2:\n",
    "    d = lexicon.analyze(i, normalize=True)\n",
    "    avg_dict2 = { k: d.get(k, 0)/len2 + avg_dict2.get(k, 0) for k in set(d) | set(avg_dict2) }\n",
    "    #d = {k: v for k, v in d.items() if v > 0}\n",
    "    \n",
    "for k, v in sorted(avg_dict2.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(list(lexicon.analyze(x_senti2[2], normalize=True).values()))\n",
    "d = lexicon.analyze(x_senti2[2], normalize=True)\n",
    "for w in sorted(d.keys(), reverse=False):\n",
    "    print(w, d[w])\n",
    "\n",
    "result = [d[key] for key in sorted(d.keys(), reverse=False)]\n",
    "print()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen = 'It is not exactly a big deal, but a huge sigh of relief to get confirmation that the movie is on the right track. I love life.'\n",
    "print(sent_tokenize(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_i = []\n",
    "for i in range(len(y_test)):\n",
    "    if(y_test[i] != y_pred[i]):\n",
    "        user_i.append(i)\n",
    "\n",
    "entry_lists = []\n",
    "path_list = test_pos_entry_path_list + test_neg_entry_path_list\n",
    "for path in path_list:\n",
    "    entry_lists.append(os.scandir(path))\n",
    "    \n",
    "users = []\n",
    "\n",
    "for list_of_entries in entry_lists:\n",
    "    for entry in list_of_entries:\n",
    "        root = etree.parse(entry.path).getroot()\n",
    "        user_id = root[0].text\n",
    "        users.append(user_id)\n",
    "\n",
    "for i in user_i:\n",
    "    print(users[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lexicon.analyze(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = ['I love beinggg retarded']\n",
    "preprocessor = NLTKPreprocessor()\n",
    "preprocess_method = 'stem'\n",
    "example2 = preprocessor.transform(example, method=preprocess_method)\n",
    "\n",
    "print(example2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
